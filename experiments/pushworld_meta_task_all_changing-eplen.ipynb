{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf37c48",
   "metadata": {},
   "source": [
    "# Meta-task PushWorld Training (All Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install \"xminigrid[baselines] @ git+https://github.com/jugheadjones10/xland-minigrid.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deda6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time #noqa\n",
    "import os #noqa\n",
    "import math # noqa\n",
    "from typing import TypedDict, Optional, Literal #noqa\n",
    "import numpy as np #noqa\n",
    "import importlib #noqa\n",
    "import os #noqa\n",
    "import imageio #noqa\n",
    "\n",
    "import jax #noqa\n",
    "import jax.numpy as jnp #noqa\n",
    "import jax.tree_util as jtu #noqa\n",
    "import flax #noqa\n",
    "import flax.linen as nn #noqa\n",
    "from flax.training import orbax_utils #noqa\n",
    "import distrax #noqa\n",
    "import orbax #noqa\n",
    "import optax #noqa\n",
    "import imageio #noqa\n",
    "import wandb #noqa\n",
    "import matplotlib.pyplot as plt #noqa\n",
    "\n",
    "from flax import struct #noqa\n",
    "from flax.typing import Dtype #noqa\n",
    "from flax.linen.dtypes import promote_dtype #noqa\n",
    "from flax.linen.initializers import glorot_normal, orthogonal, zeros_init #noqa\n",
    "from flax.training.train_state import TrainState #noqa\n",
    "from flax.jax_utils import replicate, unreplicate #noqa\n",
    "from dataclasses import asdict, dataclass #noqa\n",
    "from functools import partial #noqa\n",
    "\n",
    "import xminigrid.envs.pushworld as pushworld\n",
    "from xminigrid.envs.pushworld.benchmarks import Benchmark, BenchmarkAll\n",
    "from xminigrid.envs.pushworld.constants import Tiles, NUM_TILES, SUCCESS_REWARD, LEVEL0_ALL_SIZE\n",
    "from xminigrid.envs.pushworld.environment import Environment, EnvParams, EnvParamsT\n",
    "from xminigrid.envs.pushworld.envs.single_task_pushworld import SingleTaskPushWorldEnvironment, SingleTaskPushWorldEnvParams\n",
    "from xminigrid.envs.pushworld.envs.meta_task_pushworld import MetaTaskPushWorldEnvironment\n",
    "# Import level 0 \"all\" environments\n",
    "from xminigrid.envs.pushworld.envs.single_task_all_pushworld import SingleTaskPushWorldEnvironmentAll \n",
    "from xminigrid.envs.pushworld.envs.meta_task_all_pushworld import MetaTaskPushWorldEnvironmentAll\n",
    "from xminigrid.envs.pushworld.scripts.upload import encode_puzzle\n",
    "from xminigrid.envs.pushworld.wrappers import GoalObservationWrapper, GymAutoResetWrapper, Wrapper\n",
    "from xminigrid.envs.pushworld.types import State, TimeStep, StepType, EnvCarry, PushWorldPuzzle, PushWorldPuzzleAll\n",
    "from xminigrid.envs.pushworld.grid import get_obs_from_puzzle\n",
    "from IPython.display import Video, HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e08cd3",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model adapted from minigrid baselines:\n",
    "# https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    hidden_dim: int\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, xs, init_state):\n",
    "        seq_len, input_dim = xs.shape\n",
    "        # this init might not be optimal, for example bias for reset gate should be -1 (for now ok)\n",
    "        Wi = self.param(\"Wi\", glorot_normal(in_axis=1, out_axis=0), (self.hidden_dim * 3, input_dim), self.param_dtype)\n",
    "        Wh = self.param(\"Wh\", orthogonal(column_axis=0), (self.hidden_dim * 3, self.hidden_dim), self.param_dtype)\n",
    "        bi = self.param(\"bi\", zeros_init(), (self.hidden_dim * 3,), self.param_dtype)\n",
    "        bn = self.param(\"bn\", zeros_init(), (self.hidden_dim,), self.param_dtype)\n",
    "\n",
    "        def _step_fn(h, x):\n",
    "            igates = jnp.split(Wi @ x + bi, 3)\n",
    "            hgates = jnp.split(Wh @ h, 3)\n",
    "\n",
    "            reset = nn.sigmoid(igates[0] + hgates[0])\n",
    "            update = nn.sigmoid(igates[1] + hgates[1])\n",
    "            new = nn.tanh(igates[2] + reset * (hgates[2] + bn))\n",
    "            next_h = (1 - update) * new + update * h\n",
    "\n",
    "            return next_h, next_h\n",
    "\n",
    "        # cast to the computation dtype\n",
    "        xs, init_state, Wi, Wh, bi, bn = promote_dtype(xs, init_state, Wi, Wh, bi, bn, dtype=self.dtype)\n",
    "\n",
    "        last_state, all_states = jax.lax.scan(_step_fn, init=init_state, xs=xs)\n",
    "        return all_states, last_state\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, xs, init_state):\n",
    "        # xs: [seq_len, input_dim]\n",
    "        # init_state: [num_layers, hidden_dim]\n",
    "        outs, states = [], []\n",
    "        for layer in range(self.num_layers):\n",
    "            xs, state = GRU(self.hidden_dim, self.dtype, self.param_dtype)(xs, init_state[layer])\n",
    "            outs.append(xs)\n",
    "            states.append(state)\n",
    "\n",
    "        # sum outputs from all layers, kinda like in ResNet\n",
    "        return jnp.array(outs).sum(0), jnp.array(states)\n",
    "\n",
    "\n",
    "BatchedRNNModel = flax.linen.vmap(\n",
    "    RNNModel, variable_axes={\"params\": None}, split_rngs={\"params\": False}, axis_name=\"batch\"\n",
    ")\n",
    "\n",
    "\n",
    "class EmbeddingEncoder(nn.Module):\n",
    "    emb_dim: int = 16\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, img):\n",
    "        entity_emb = nn.Embed(NUM_TILES, self.emb_dim, self.dtype, self.param_dtype)\n",
    "\n",
    "        # [..., channels]\n",
    "        img_emb = entity_emb(img[..., 0])\n",
    "        return img_emb\n",
    "\n",
    "\n",
    "class ActorCriticInput(TypedDict):\n",
    "    obs: jax.Array\n",
    "    prev_action: jax.Array\n",
    "    prev_reward: jax.Array\n",
    "\n",
    "\n",
    "class ActorCriticRNN(nn.Module):\n",
    "    num_actions: int\n",
    "    obs_emb_dim: int = 16\n",
    "    action_emb_dim: int = 16\n",
    "    rnn_hidden_dim: int = 64\n",
    "    rnn_num_layers: int = 1\n",
    "    head_hidden_dim: int = 64\n",
    "    img_obs: bool = False\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: ActorCriticInput, hidden: jax.Array) -> tuple[distrax.Categorical, jax.Array, jax.Array]:\n",
    "        B, S = inputs[\"obs\"].shape[:2]\n",
    "\n",
    "        # encoder from https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n",
    "        if self.img_obs:\n",
    "            img_encoder = nn.Sequential(\n",
    "                [\n",
    "                    nn.Conv(\n",
    "                        16,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            img_encoder = nn.Sequential(\n",
    "                [\n",
    "                    # For small dims nn.Embed is extremely slow in bf16, so we leave everything in default dtypes\n",
    "                    EmbeddingEncoder(emb_dim=self.obs_emb_dim),\n",
    "                    nn.Conv(\n",
    "                        16,\n",
    "                        (2, 2),\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (2, 2),\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        64,\n",
    "                        (2, 2),\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                ]\n",
    "            )\n",
    "        action_encoder = nn.Embed(self.num_actions, self.action_emb_dim)\n",
    "\n",
    "        rnn_core = BatchedRNNModel(\n",
    "            self.rnn_hidden_dim, self.rnn_num_layers, dtype=self.dtype, param_dtype=self.param_dtype\n",
    "        )\n",
    "        actor = nn.Sequential(\n",
    "            [\n",
    "                nn.Dense(\n",
    "                    self.head_hidden_dim, kernel_init=orthogonal(2), dtype=self.dtype, param_dtype=self.param_dtype\n",
    "                ),\n",
    "                nn.tanh,\n",
    "                nn.Dense(\n",
    "                    self.num_actions, kernel_init=orthogonal(0.01), dtype=self.dtype, param_dtype=self.param_dtype\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        critic = nn.Sequential(\n",
    "            [\n",
    "                nn.Dense(\n",
    "                    self.head_hidden_dim, kernel_init=orthogonal(2), dtype=self.dtype, param_dtype=self.param_dtype\n",
    "                ),\n",
    "                nn.tanh,\n",
    "                nn.Dense(1, kernel_init=orthogonal(1.0), dtype=self.dtype, param_dtype=self.param_dtype),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # [batch_size, seq_len, ...]\n",
    "        obs_emb = img_encoder(inputs[\"obs\"].astype(jnp.int32)).reshape(B, S, -1)\n",
    "        act_emb = action_encoder(inputs[\"prev_action\"])\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim + act_emb_dim + 1]\n",
    "        out = jnp.concatenate([obs_emb, act_emb, inputs[\"prev_reward\"][..., None]], axis=-1)\n",
    "\n",
    "        # core networks\n",
    "        out, new_hidden = rnn_core(out, hidden)\n",
    "\n",
    "        # casting to full precision for the loss, as softmax/log_softmax\n",
    "        # (inside Categorical) is not stable in bf16\n",
    "        logits = actor(out).astype(jnp.float32)\n",
    "\n",
    "        dist = distrax.Categorical(logits=logits)\n",
    "        values = critic(out)\n",
    "\n",
    "        return dist, jnp.squeeze(values, axis=-1), new_hidden\n",
    "\n",
    "    def initialize_carry(self, batch_size):\n",
    "        return jnp.zeros((batch_size, self.rnn_num_layers, self.rnn_hidden_dim), dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74a956",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for PPO training and evaluation\n",
    "\n",
    "\n",
    "# Training stuff\n",
    "class Transition(struct.PyTreeNode):\n",
    "    done: jax.Array\n",
    "    action: jax.Array\n",
    "    value: jax.Array\n",
    "    reward: jax.Array\n",
    "    log_prob: jax.Array\n",
    "    # for obs\n",
    "    obs: jax.Array\n",
    "    # for rnn policy\n",
    "    prev_action: jax.Array\n",
    "    prev_reward: jax.Array\n",
    "\n",
    "\n",
    "def calculate_gae(\n",
    "    transitions: Transition,\n",
    "    last_val: jax.Array,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "    # single iteration for the loop\n",
    "    def _get_advantages(gae_and_next_value, transition):\n",
    "        gae, next_value = gae_and_next_value\n",
    "        delta = transition.reward + gamma * next_value * (1 - transition.done) - transition.value\n",
    "        gae = delta + gamma * gae_lambda * (1 - transition.done) * gae\n",
    "        return (gae, transition.value), gae\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        _get_advantages,\n",
    "        (jnp.zeros_like(last_val), last_val),\n",
    "        transitions,\n",
    "        reverse=True,\n",
    "    )\n",
    "    # advantages and values (Q)\n",
    "    return advantages, advantages + transitions.value\n",
    "\n",
    "\n",
    "def ppo_update_networks(\n",
    "    train_state: TrainState,\n",
    "    transitions: Transition,\n",
    "    init_hstate: jax.Array,\n",
    "    advantages: jax.Array,\n",
    "    targets: jax.Array,\n",
    "    clip_eps: float,\n",
    "    vf_coef: float,\n",
    "    ent_coef: float,\n",
    "):\n",
    "    # NORMALIZE ADVANTAGES\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    def _loss_fn(params):\n",
    "        # RERUN NETWORK\n",
    "        dist, value, _ = train_state.apply_fn(\n",
    "            params,\n",
    "            {\n",
    "                # [batch_size, seq_len, ...]\n",
    "                \"obs\": transitions.obs,\n",
    "                \"prev_action\": transitions.prev_action,\n",
    "                \"prev_reward\": transitions.prev_reward,\n",
    "            },\n",
    "            init_hstate,\n",
    "        )\n",
    "        log_prob = dist.log_prob(transitions.action)\n",
    "\n",
    "        # CALCULATE VALUE LOSS\n",
    "        value_pred_clipped = transitions.value + (value - transitions.value).clip(-clip_eps, clip_eps)\n",
    "        value_loss = jnp.square(value - targets)\n",
    "        value_loss_clipped = jnp.square(value_pred_clipped - targets)\n",
    "        value_loss = 0.5 * jnp.maximum(value_loss, value_loss_clipped).mean()\n",
    "\n",
    "        # TODO: ablate this!\n",
    "        # value_loss = jnp.square(value - targets).mean()\n",
    "\n",
    "        # CALCULATE ACTOR LOSS\n",
    "        ratio = jnp.exp(log_prob - transitions.log_prob)\n",
    "        actor_loss1 = advantages * ratio\n",
    "        actor_loss2 = advantages * jnp.clip(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "        actor_loss = -jnp.minimum(actor_loss1, actor_loss2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        total_loss = actor_loss + vf_coef * value_loss - ent_coef * entropy\n",
    "        return total_loss, (value_loss, actor_loss, entropy)\n",
    "\n",
    "    (loss, (vloss, aloss, entropy)), grads = jax.value_and_grad(_loss_fn, has_aux=True)(train_state.params)\n",
    "    (loss, vloss, aloss, entropy, grads) = jax.lax.pmean((loss, vloss, aloss, entropy, grads), axis_name=\"devices\")\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    update_info = {\n",
    "        \"total_loss\": loss,\n",
    "        \"value_loss\": vloss,\n",
    "        \"actor_loss\": aloss,\n",
    "        \"entropy\": entropy,\n",
    "    }\n",
    "    return train_state, update_info\n",
    "\n",
    "\n",
    "# for evaluation (evaluate for N consecutive episodes, sum rewards)\n",
    "# N=1 single task, N>1 for meta-RL\n",
    "class RolloutStats(struct.PyTreeNode):\n",
    "    reward: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0.0))\n",
    "    length: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0))\n",
    "    episodes: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0))\n",
    "    solved: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0))\n",
    "\n",
    "\n",
    "# for tracking per-episode statistics during meta-RL evaluation\n",
    "class MetaRolloutStats(struct.PyTreeNode):\n",
    "    episode_rewards: jax.Array  # Shape: [max_episodes] - reward for each episode\n",
    "    episode_lengths: jax.Array  # Shape: [max_episodes] - length of each episode\n",
    "    episode_solved: jax.Array  # Shape: [max_episodes] - whether each episode was solved\n",
    "    total_reward: jax.Array  # Scalar - total reward across all episodes\n",
    "    num_episodes_completed: jax.Array  # Scalar - number of episodes actually completed\n",
    "\n",
    "\n",
    "def rollout(\n",
    "    rng: jax.Array,\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    eval_puzzle: PushWorldPuzzleAll,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: jax.Array,\n",
    "    num_consecutive_episodes: int = 1,\n",
    ") -> RolloutStats:\n",
    "    def _cond_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate = carry\n",
    "        return jnp.less(stats.episodes, num_consecutive_episodes)\n",
    "\n",
    "    def _body_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate = carry\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hstate = train_state.apply_fn(\n",
    "            train_state.params,\n",
    "            {\n",
    "                # We add single channel dimension to end of obs\n",
    "                \"obs\": timestep.observation[None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "        timestep = env.step(env_params, timestep, action)\n",
    "\n",
    "        solved_flag = ((timestep.reward == SUCCESS_REWARD) & (timestep.last() == 1)).astype(jnp.int32)\n",
    "        stats = stats.replace(\n",
    "            reward=stats.reward + timestep.reward,\n",
    "            length=stats.length + 1,\n",
    "            episodes=stats.episodes + timestep.last(),\n",
    "            solved=solved_flag,\n",
    "        )\n",
    "        carry = (rng, stats, timestep, action, timestep.reward, hstate)\n",
    "        return carry\n",
    "\n",
    "    env_params = env_params.replace(puzzle=eval_puzzle)\n",
    "    timestep = env.eval_reset(env_params, rng)\n",
    "    prev_action = jnp.asarray(0)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    init_carry = (rng, RolloutStats(), timestep, prev_action, prev_reward, init_hstate)\n",
    "\n",
    "    final_carry = jax.lax.while_loop(_cond_fn, _body_fn, init_val=init_carry)\n",
    "    return final_carry[1]\n",
    "\n",
    "\n",
    "def meta_rollout(\n",
    "    rng: jax.Array,\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    eval_puzzle: PushWorldPuzzleAll,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: jax.Array,\n",
    "    num_consecutive_episodes: int = 1,\n",
    ") -> MetaRolloutStats:\n",
    "    \"\"\"Rollout that tracks statistics for each individual episode.\"\"\"\n",
    "\n",
    "    def _cond_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate, current_episode_reward, current_episode_length = carry\n",
    "        return jnp.less(stats.num_episodes_completed, num_consecutive_episodes)\n",
    "\n",
    "    def _body_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate, current_episode_reward, current_episode_length = carry\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hstate = train_state.apply_fn(\n",
    "            train_state.params,\n",
    "            {\n",
    "                # We add single channel dimension to end of obs_img\n",
    "                \"obs\": timestep.observation[None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "        timestep = env.step(env_params, timestep, action)\n",
    "\n",
    "        # Update current episode accumulators\n",
    "        current_episode_reward = current_episode_reward + timestep.reward\n",
    "        current_episode_length = current_episode_length + 1\n",
    "\n",
    "        # Check if episode ended\n",
    "        episode_ended = timestep.last()\n",
    "        solved_flag = ((timestep.reward == SUCCESS_REWARD) & (episode_ended == 1)).astype(jnp.int32)\n",
    "\n",
    "        # When episode ends, store the episode stats\n",
    "        episode_idx = stats.num_episodes_completed\n",
    "        new_episode_rewards = stats.episode_rewards.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, current_episode_reward, stats.episode_rewards[episode_idx])\n",
    "        )\n",
    "        new_episode_lengths = stats.episode_lengths.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, current_episode_length, stats.episode_lengths[episode_idx])\n",
    "        )\n",
    "        new_episode_solved = stats.episode_solved.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, solved_flag, stats.episode_solved[episode_idx])\n",
    "        )\n",
    "\n",
    "        # Update stats\n",
    "        stats = stats.replace(\n",
    "            episode_rewards=new_episode_rewards,\n",
    "            episode_lengths=new_episode_lengths,\n",
    "            episode_solved=new_episode_solved,\n",
    "            total_reward=stats.total_reward + timestep.reward,\n",
    "            num_episodes_completed=stats.num_episodes_completed + episode_ended,\n",
    "        )\n",
    "\n",
    "        # Reset episode accumulators when episode ends\n",
    "        current_episode_reward = jnp.where(episode_ended, 0.0, current_episode_reward)\n",
    "        current_episode_length = jnp.where(episode_ended, 0, current_episode_length)\n",
    "\n",
    "        carry = (rng, stats, timestep, action, timestep.reward, hstate, current_episode_reward, current_episode_length)\n",
    "        return carry\n",
    "\n",
    "    # Initialize episode tracking arrays\n",
    "    episode_rewards = jnp.zeros(num_consecutive_episodes)\n",
    "    episode_lengths = jnp.zeros(num_consecutive_episodes, dtype=jnp.int32)\n",
    "    episode_solved = jnp.zeros(num_consecutive_episodes, dtype=jnp.int32)\n",
    "\n",
    "    initial_stats = MetaRolloutStats(\n",
    "        episode_rewards=episode_rewards,\n",
    "        episode_lengths=episode_lengths,\n",
    "        episode_solved=episode_solved,\n",
    "        total_reward=jnp.asarray(0.0),\n",
    "        num_episodes_completed=jnp.asarray(0),\n",
    "    )\n",
    "\n",
    "    env_params = env_params.replace(puzzle=eval_puzzle)\n",
    "    timestep = env.eval_reset(env_params, rng)\n",
    "    prev_action = jnp.asarray(0)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    current_episode_reward = jnp.asarray(0.0)\n",
    "    current_episode_length = jnp.asarray(0)\n",
    "\n",
    "    init_carry = (\n",
    "        rng,\n",
    "        initial_stats,\n",
    "        timestep,\n",
    "        prev_action,\n",
    "        prev_reward,\n",
    "        init_hstate,\n",
    "        current_episode_reward,\n",
    "        current_episode_length,\n",
    "    )\n",
    "\n",
    "    final_carry = jax.lax.while_loop(_cond_fn, _body_fn, init_val=init_carry)\n",
    "    return final_carry[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc6958",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_threefry_partitionable\", True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    project: str = \"PushWorld\"\n",
    "    group: str = \"default\"\n",
    "    name: str = \"meta-task-ppo\"\n",
    "    benchmark_id: str = \"level0_transformed_all\"\n",
    "    track: bool = False\n",
    "    checkpoint_path: Optional[str] = None\n",
    "    upload_model: bool = False\n",
    "\n",
    "    train_test_same: bool = False\n",
    "    num_train: Optional[int] = None\n",
    "    num_test: Optional[int] = None\n",
    "\n",
    "    img_obs: bool = False\n",
    "    obs_emb_dim: int = 16\n",
    "    action_emb_dim: int = 16\n",
    "    rnn_hidden_dim: int = 1024\n",
    "    rnn_num_layers: int = 1\n",
    "    head_hidden_dim: int = 256\n",
    "    enable_bf16: bool = False\n",
    "    num_envs: int = 8192\n",
    "    num_steps_per_env: int = 4096\n",
    "    num_steps_per_update: int = 32\n",
    "    update_epochs: int = 1\n",
    "    num_minibatches: int = 16\n",
    "    total_timesteps: int = 100_000_000\n",
    "    lr: float = 0.001\n",
    "    clip_eps: float = 0.2\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    eval_num_envs: int = 512\n",
    "    eval_num_episodes: int = 10\n",
    "    eval_seed: int = 42\n",
    "    train_seed: int = 42\n",
    "    puzzle_seed: int = 42\n",
    "    checkpoint_path: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        num_devices = jax.local_device_count()\n",
    "        assert self.num_envs % num_devices == 0\n",
    "\n",
    "        self.num_envs_per_device = self.num_envs // num_devices\n",
    "\n",
    "        self.meta_updates_per_schedule = 10\n",
    "        self.steps_schedule = [100, 200, 300, 400, 500]\n",
    "        self.max_steps = max(self.steps_schedule)\n",
    "        self.total_timesteps_per_device = sum(self.steps_schedule) * self.meta_updates_per_schedule\n",
    "\n",
    "        self.num_meta_updates = self.meta_updates_per_schedule * len(self.steps_schedule)\n",
    "        self.num_inner_updates = 1\n",
    "\n",
    "        print(f\"Num devices: {num_devices}, Num meta updates: {self.num_meta_updates}\")\n",
    "\n",
    "\n",
    "def make_states(config: TrainConfig):\n",
    "    # for learning rage scheduling\n",
    "    def linear_schedule(count):\n",
    "        total_inner_updates = config.num_minibatches * config.update_epochs * config.num_inner_updates\n",
    "        frac = 1.0 - (count // total_inner_updates) / config.num_meta_updates\n",
    "        return config.lr * frac\n",
    "\n",
    "\n",
    "    env = MetaTaskPushWorldEnvironmentAll()\n",
    "    env_params = env.default_params()\n",
    "    env = GymAutoResetWrapper(env)\n",
    "\n",
    "\n",
    "    benchmark = pushworld.load_all_benchmark(config.benchmark_id)\n",
    "\n",
    "    puzzle_rng = jax.random.key(config.puzzle_seed)\n",
    "    train_rng, test_rng = jax.random.split(puzzle_rng)\n",
    "\n",
    "    if config.num_train is not None:\n",
    "        assert config.num_train <= benchmark.num_train_puzzles(), (\n",
    "            \"num_train is larger than num train available in benchmark\"\n",
    "        )\n",
    "        perm = jax.random.permutation(train_rng, benchmark.num_train_puzzles())\n",
    "        idxs = perm[: config.num_train]\n",
    "        benchmark = benchmark.replace(train_puzzles=benchmark.train_puzzles[idxs])\n",
    "    else:\n",
    "        config.num_train = benchmark.num_train_puzzles()\n",
    "\n",
    "    if config.num_test is not None:\n",
    "        assert config.num_test <= benchmark.num_test_puzzles(), (\n",
    "            \"num_test is larger than num test available in benchmark\"\n",
    "        )\n",
    "        perm = jax.random.permutation(test_rng, benchmark.num_test_puzzles())\n",
    "        idxs = perm[: config.num_test]\n",
    "        benchmark = benchmark.replace(test_puzzles=benchmark.test_puzzles[idxs])\n",
    "    else:\n",
    "        config.num_test = benchmark.num_test_puzzles()\n",
    "\n",
    "    if config.train_test_same:\n",
    "        benchmark = benchmark.replace(test_puzzles=benchmark.train_puzzles)\n",
    "        config.num_test = config.num_train\n",
    "\n",
    "    rng = jax.random.key(config.train_seed)\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    network = ActorCriticRNN(\n",
    "        num_actions=env.num_actions(env_params),\n",
    "        obs_emb_dim=config.obs_emb_dim,\n",
    "        action_emb_dim=config.action_emb_dim,\n",
    "        rnn_hidden_dim=config.rnn_hidden_dim,\n",
    "        rnn_num_layers=config.rnn_num_layers,\n",
    "        head_hidden_dim=config.head_hidden_dim,\n",
    "        img_obs=config.img_obs,\n",
    "        dtype=jnp.bfloat16 if config.enable_bf16 else None,\n",
    "    )\n",
    "    shapes = env.observation_shape(env_params)\n",
    "\n",
    "    init_obs = {\n",
    "        # We add single channel dimension to end of obs_img\n",
    "        \"obs\": jnp.zeros((config.num_envs_per_device, 1, *shapes)),\n",
    "        \"prev_action\": jnp.zeros((config.num_envs_per_device, 1), dtype=jnp.int32),\n",
    "        \"prev_reward\": jnp.zeros((config.num_envs_per_device, 1)),\n",
    "    }\n",
    "    init_hstate = network.initialize_carry(batch_size=config.num_envs_per_device)\n",
    "\n",
    "    network_params = network.init(_rng, init_obs, init_hstate)\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(config.max_grad_norm),\n",
    "        optax.inject_hyperparams(optax.adam)(learning_rate=linear_schedule, eps=1e-8),  # eps=1e-5\n",
    "    )\n",
    "    train_state = TrainState.create(apply_fn=network.apply, params=network_params, tx=tx)\n",
    "\n",
    "    return rng, env, env_params, benchmark, init_hstate, train_state\n",
    "\n",
    "\n",
    "def make_train(\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    benchmark: BenchmarkAll,\n",
    "    config: TrainConfig,\n",
    "):\n",
    "    @partial(jax.pmap, axis_name=\"devices\")\n",
    "    def train(\n",
    "        rng: jax.Array,\n",
    "        train_state: TrainState,\n",
    "        init_hstate: jax.Array,\n",
    "    ):\n",
    "        eval_hstate = init_hstate[0][None]\n",
    "\n",
    "        # META TRAIN LOOP\n",
    "        # @partial(jax.jit, static_argnums=(1,))\n",
    "        def _meta_step(meta_state, num_steps_per_update):\n",
    "            rng, train_state = meta_state\n",
    "\n",
    "            rng, _rng1, _rng2 = jax.random.split(rng, num=3)\n",
    "            puzzle_rng = jax.random.split(_rng1, num=config.num_envs_per_device)\n",
    "            reset_rng = jax.random.split(_rng2, num=config.num_envs_per_device)\n",
    "\n",
    "            puzzles = jax.vmap(benchmark.sample_puzzle, in_axes=(0, None))(puzzle_rng, \"train\")\n",
    "            meta_env_params = env_params.replace(puzzle=puzzles)\n",
    "\n",
    "            timestep = jax.vmap(env.reset, in_axes=(0, 0))(meta_env_params, reset_rng)\n",
    "            prev_action = jnp.zeros(config.num_envs_per_device, dtype=jnp.int32)\n",
    "            prev_reward = jnp.zeros(config.num_envs_per_device)\n",
    "\n",
    "            # INNER TRAIN LOOP\n",
    "            def _update_step(runner_state, _):\n",
    "                # COLLECT TRAJECTORIES\n",
    "                def _env_step(runner_state, _):\n",
    "                    rng, train_state, prev_timestep, prev_action, prev_reward, prev_hstate = runner_state\n",
    "\n",
    "                    rng, _rng = jax.random.split(rng)\n",
    "                    dist, value, hstate = train_state.apply_fn(\n",
    "                        train_state.params,\n",
    "                        {\n",
    "                            # [batch_size, seq_len=1, ...]\n",
    "                            # We add single channel dimension to end of obs_img\n",
    "                            \"obs\": prev_timestep.observation[:, None],\n",
    "                            \"prev_action\": prev_action[:, None],\n",
    "                            \"prev_reward\": prev_reward[:, None],\n",
    "                        },\n",
    "                        prev_hstate,\n",
    "                    )\n",
    "                    action, log_prob = dist.sample_and_log_prob(seed=_rng)\n",
    "                    action, value, log_prob = action.squeeze(1), value.squeeze(1), log_prob.squeeze(1)\n",
    "\n",
    "                    timestep = jax.vmap(env.step, in_axes=0)(meta_env_params, prev_timestep, action)\n",
    "                    transition = Transition(\n",
    "                        # ATTENTION: done is always false, as we optimize for entire meta-rollout\n",
    "                        done=jnp.zeros_like(timestep.last()),\n",
    "                        action=action,\n",
    "                        value=value,\n",
    "                        reward=timestep.reward,\n",
    "                        log_prob=log_prob,\n",
    "                        obs=prev_timestep.observation,\n",
    "                        prev_action=prev_action,\n",
    "                        prev_reward=prev_reward,\n",
    "                    )\n",
    "                    runner_state = (rng, train_state, timestep, action, timestep.reward, hstate)\n",
    "                    return runner_state, transition\n",
    "\n",
    "                initial_hstate = runner_state[-1]\n",
    "                runner_state, transitions = jax.lax.scan(_env_step, runner_state, None, num_steps_per_update)\n",
    "\n",
    "                rng, train_state, timestep, prev_action, prev_reward, hstate = runner_state\n",
    "                _, last_val, _ = train_state.apply_fn(\n",
    "                    train_state.params,\n",
    "                    {\n",
    "                        # We add single channel dimension to end of obs_img\n",
    "                        \"obs\": timestep.observation[:, None],\n",
    "                        \"prev_action\": prev_action[:, None],\n",
    "                        \"prev_reward\": prev_reward[:, None],\n",
    "                    },\n",
    "                    hstate,\n",
    "                )\n",
    "                advantages, targets = calculate_gae(transitions, last_val.squeeze(1), config.gamma, config.gae_lambda)\n",
    "\n",
    "                # UPDATE NETWORK\n",
    "                def _update_epoch(update_state, _):\n",
    "                    def _update_minbatch(train_state, batch_info):\n",
    "                        init_hstate, transitions, advantages, targets = batch_info\n",
    "                        new_train_state, update_info = ppo_update_networks(\n",
    "                            train_state=train_state,\n",
    "                            transitions=transitions,\n",
    "                            init_hstate=init_hstate.squeeze(1),\n",
    "                            advantages=advantages,\n",
    "                            targets=targets,\n",
    "                            clip_eps=config.clip_eps,\n",
    "                            vf_coef=config.vf_coef,\n",
    "                            ent_coef=config.ent_coef,\n",
    "                        )\n",
    "                        return new_train_state, update_info\n",
    "\n",
    "                    rng, train_state, init_hstate, transitions, advantages, targets = update_state\n",
    "\n",
    "                    rng, _rng = jax.random.split(rng)\n",
    "                    permutation = jax.random.permutation(_rng, config.num_envs_per_device)\n",
    "                    batch = (init_hstate, transitions, advantages, targets)\n",
    "                    batch = jtu.tree_map(lambda x: x.swapaxes(0, 1), batch)\n",
    "\n",
    "                    shuffled_batch = jtu.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "                    minibatches = jtu.tree_map(\n",
    "                        lambda x: jnp.reshape(x, (config.num_minibatches, -1) + x.shape[1:]), shuffled_batch\n",
    "                    )\n",
    "                    train_state, update_info = jax.lax.scan(_update_minbatch, train_state, minibatches)\n",
    "\n",
    "                    update_state = (rng, train_state, init_hstate, transitions, advantages, targets)\n",
    "                    return update_state, update_info\n",
    "\n",
    "                update_state = (rng, train_state, initial_hstate[None, :], transitions, advantages, targets)\n",
    "                update_state, loss_info = jax.lax.scan(_update_epoch, update_state, None, config.update_epochs)\n",
    "                rng, train_state = update_state[:2]\n",
    "\n",
    "                loss_info = jtu.tree_map(lambda x: x.mean(-1).mean(-1), loss_info)\n",
    "                runner_state = (rng, train_state, timestep, prev_action, prev_reward, hstate)\n",
    "                return runner_state, loss_info\n",
    "\n",
    "            runner_state = (rng, train_state, timestep, prev_action, prev_reward, init_hstate)\n",
    "            runner_state, loss_info = jax.lax.scan(_update_step, runner_state, None, config.num_inner_updates)\n",
    "            rng, train_state = runner_state[:2]\n",
    "\n",
    "            eval_reset_rng = jax.random.key(config.eval_seed)\n",
    "            eval_test_rng, eval_train_rng = jax.random.split(eval_reset_rng)\n",
    "            assert config.num_test is not None, \"num_test must be set for evaluation\"\n",
    "            assert config.num_train is not None, \"num_train must be set for evaluation\"\n",
    "\n",
    "            eval_test_reset_rng = jax.random.split(eval_test_rng, num=config.num_test)\n",
    "            eval_test_puzzles = benchmark.get_test_puzzles()\n",
    "            eval_test_stats = jax.vmap(meta_rollout, in_axes=(0, None, None, 0, None, None, None))(\n",
    "                eval_test_reset_rng,\n",
    "                env,\n",
    "                meta_env_params,\n",
    "                eval_test_puzzles,\n",
    "                train_state,\n",
    "                eval_hstate,\n",
    "                config.eval_num_episodes,\n",
    "            )\n",
    "            eval_test_stats = jax.lax.pmean(eval_test_stats, axis_name=\"devices\")\n",
    "\n",
    "            eval_train_reset_rng = jax.random.split(eval_train_rng, num=config.num_train)\n",
    "            eval_train_puzzles = benchmark.get_train_puzzles()\n",
    "            eval_train_stats = jax.vmap(meta_rollout, in_axes=(0, None, None, 0, None, None, None))(\n",
    "                eval_train_reset_rng,\n",
    "                env,\n",
    "                meta_env_params,\n",
    "                eval_train_puzzles,\n",
    "                train_state,\n",
    "                eval_hstate,\n",
    "                config.eval_num_episodes,\n",
    "            )\n",
    "            eval_train_stats = jax.lax.pmean(eval_train_stats, axis_name=\"devices\")\n",
    "\n",
    "            loss_info = jtu.tree_map(lambda x: x.mean(-1), loss_info)\n",
    "            loss_info.update(\n",
    "                {\n",
    "                    # Originally we divided returns_mean by config.eval_num_episodes, but we realized\n",
    "                    # that it is more intuitive to think about the cumulative returns over the whole meta-episode.\n",
    "                    \"eval_test/returns_mean\": eval_test_stats.total_reward.mean(),\n",
    "                    \"eval_train/returns_mean_train\": eval_train_stats.total_reward.mean(),\n",
    "                    \"eval_test/returns_median\": jnp.median(eval_test_stats.total_reward),\n",
    "                    \"eval_test/returns_20percentile\": jnp.percentile(eval_test_stats.total_reward, q=20),\n",
    "                    # Our definition of solved is whether the agent solves the last trial in the meta-episode.\n",
    "                    \"eval_test/solved_percentage\": eval_test_stats.episode_solved[:, -1].mean(),\n",
    "                    \"eval_train/solved_percentage_train\": eval_train_stats.episode_solved[:, -1].mean(),\n",
    "                    \"eval_test/lengths\": eval_test_stats.episode_lengths.mean(),\n",
    "                    \"eval_test/lengths_20percentile\": jnp.percentile(eval_test_stats.episode_lengths, q=20),\n",
    "                    \"lr\": train_state.opt_state[-1].hyperparams[\"learning_rate\"],\n",
    "                    # Store episode arrays - we'll convert to individual metrics outside JIT\n",
    "                    \"eval_test/episode_rewards\": eval_test_stats.episode_rewards.mean(axis=0),\n",
    "                    \"eval_test/episode_solved_rates\": eval_test_stats.episode_solved.mean(axis=0),\n",
    "                    \"eval_train/episode_rewards\": eval_train_stats.episode_rewards.mean(axis=0),\n",
    "                    \"eval_train/episode_solved_rates\": eval_train_stats.episode_solved.mean(axis=0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            meta_state = (rng, train_state)\n",
    "            return meta_state, loss_info\n",
    "\n",
    "        schedule_list = []\n",
    "        for num_steps in config.steps_schedule:\n",
    "            schedule_list.extend([num_steps] * config.meta_updates_per_schedule)\n",
    "\n",
    "        meta_state = (rng, train_state)\n",
    "        all_loss_info = []\n",
    "        for num_steps in schedule_list:\n",
    "            meta_state, loss_info = _meta_step(meta_state, num_steps)\n",
    "            all_loss_info.append(loss_info)\n",
    "\n",
    "        return {\"state\": meta_state[-1], \"loss_info\": all_loss_info}\n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "def train(config: TrainConfig):\n",
    "    # removing existing checkpoints if any\n",
    "    if config.checkpoint_path is not None and os.path.exists(config.checkpoint_path):\n",
    "        shutil.rmtree(config.checkpoint_path)\n",
    "\n",
    "    rng, env, env_params, benchmark, init_hstate, train_state = make_states(config)\n",
    "    rng = jax.random.split(rng, num=jax.local_device_count())\n",
    "    train_state = replicate(train_state, jax.local_devices())\n",
    "    init_hstate = replicate(init_hstate, jax.local_devices())\n",
    "\n",
    "    print(\"Compiling...\")\n",
    "    t = time.time()\n",
    "    train_fn = make_train(env, env_params, benchmark, config)\n",
    "    train_fn = train_fn.lower(rng, train_state, init_hstate).compile()\n",
    "    elapsed_time = time.time() - t\n",
    "    print(f\"Done in {elapsed_time:.2f}s.\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    t = time.time()\n",
    "    train_info = jax.block_until_ready(train_fn(rng, train_state, init_hstate))\n",
    "    elapsed_time = time.time() - t\n",
    "    print(f\"Done in {elapsed_time:.2f}s.\")\n",
    "\n",
    "    return unreplicate(train_info), elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1d9d0",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ef584",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_threefry_partitionable\", True)\n",
    "\n",
    "\n",
    "def processing(config: TrainConfig, train_info, elapsed_time):\n",
    "    print(\"Logginig...\")\n",
    "    loss_info = train_info[\"loss_info\"]\n",
    "\n",
    "    if config.track or config.upload_model:\n",
    "        run = wandb.init(\n",
    "            project=config.project,\n",
    "            group=config.group,\n",
    "            name=config.name,\n",
    "            config=asdict(config),\n",
    "            save_code=True,\n",
    "        )\n",
    "\n",
    "    if config.track:\n",
    "        total_transitions = 0\n",
    "\n",
    "        # I want to manipulate episode_rewards and episode_solved_rates so that I generate\n",
    "        # eval_num_episodes separate metrics, where each metric represents the returns or solved rate\n",
    "        # for one episode in the meta-episode, over meta_updates.\n",
    "        for episode_idx in range(config.eval_num_episodes):\n",
    "            loss_info[f\"eval_test/episode_rewards/{episode_idx}\"] = loss_info[\"eval_test/episode_rewards\"].swapaxes(\n",
    "                0, 1\n",
    "            )[episode_idx]\n",
    "            loss_info[f\"eval_test/episode_solved_rates/{episode_idx}\"] = loss_info[\n",
    "                \"eval_test/episode_solved_rates\"\n",
    "            ].swapaxes(0, 1)[episode_idx]\n",
    "            loss_info[f\"eval_train/episode_rewards/{episode_idx}\"] = loss_info[\"eval_train/episode_rewards\"].swapaxes(\n",
    "                0, 1\n",
    "            )[episode_idx]\n",
    "            loss_info[f\"eval_train/episode_solved_rates/{episode_idx}\"] = loss_info[\n",
    "                \"eval_train/episode_solved_rates\"\n",
    "            ].swapaxes(0, 1)[episode_idx]\n",
    "\n",
    "        loss_info.pop(\"eval_test/episode_rewards\")\n",
    "        loss_info.pop(\"eval_test/episode_solved_rates\")\n",
    "        loss_info.pop(\"eval_train/episode_rewards\")\n",
    "        loss_info.pop(\"eval_train/episode_solved_rates\")\n",
    "\n",
    "        for i in range(config.num_meta_updates):\n",
    "            info = jtu.tree_map(lambda x: x[i].item(), loss_info)\n",
    "            wandb.log(info)\n",
    "\n",
    "        run.summary[\"training_time\"] = elapsed_time\n",
    "        run.summary[\"steps_per_second\"] = (config.total_timesteps_per_device * jax.local_device_count()) / elapsed_time\n",
    "\n",
    "    if config.checkpoint_path is not None:\n",
    "        checkpoint = {\"config\": asdict(config), \"params\": train_info[\"state\"].params}\n",
    "        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        save_args = orbax_utils.save_args_from_target(checkpoint)\n",
    "        orbax_checkpointer.save(config.checkpoint_path, checkpoint, save_args=save_args)\n",
    "\n",
    "        if config.upload_model:\n",
    "            artifact = wandb.Artifact(\n",
    "                name=f\"model-checkpoint-{run.id}\", type=\"model\", description=\"Trained model checkpoint\"\n",
    "            )\n",
    "            artifact.add_dir(config.checkpoint_path)  # Add entire checkpoint directory\n",
    "            run.log_artifact(artifact)\n",
    "\n",
    "    if config.track or config.upload_model:\n",
    "        run.finish()\n",
    "\n",
    "    print(\"Final test return: \", float(loss_info[\"eval_test/returns_mean\"][-1]))\n",
    "    print(\"Final train return: \", float(loss_info[\"eval_train/returns_mean_train\"][-1]))\n",
    "    print(\"Final test solve rate: \", float(loss_info[\"eval_test/solved_percentage\"][-1]))\n",
    "    print(\"Final train solve rate: \", float(loss_info[\"eval_train/solved_percentage_train\"][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573feee",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_string: str):\n",
    "    \"\"\"Converts a standard 6-digit hex color into a tuple of decimal\n",
    "    (red, green, blue) values.\"\"\"\n",
    "    return tuple(int(hex_string[i : i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "\n",
    "symbol_to_rgb = {\n",
    "    0: hex_to_rgb(\"FFFFFF\"),  # empty → white\n",
    "    1: hex_to_rgb(\"00DC00\"),  # agent → \"00DC00\"\n",
    "    2: hex_to_rgb(\"469BFF\"),  # movable → \"469BFF\"\n",
    "    3: hex_to_rgb(\"DC0000\"),  # movable_goal → \"DC0000\"\n",
    "    4: hex_to_rgb(\"0A0A0A\"),  # wall → \"0A0A0A\"\n",
    "}\n",
    "\n",
    "\n",
    "def text_to_rgb(goal_pos, grid):\n",
    "    \"\"\"grid: 2-D array of str, shape (H, W)\"\"\"\n",
    "    h, w = grid.shape\n",
    "    img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for sym, rgb in symbol_to_rgb.items():\n",
    "        mask = grid == sym\n",
    "        img[mask] = rgb\n",
    "\n",
    "    if grid[goal_pos[1], goal_pos[0]] == Tiles.EMPTY:\n",
    "        img[goal_pos[1], goal_pos[0]] = hex_to_rgb(\"FF7F7F\")  # light red\n",
    "\n",
    "    # upscale (optional) so each tile is, say, 16×16 pixels\n",
    "    img = np.kron(img, np.ones((64, 64, 1), dtype=np.uint8))\n",
    "    return img\n",
    "\n",
    "\n",
    "def text_to_rgb_all(observation: jax.Array):\n",
    "    # I want you to render the observation into a grid\n",
    "    # Observation is a jax.Array, shape (H, W, 8),\n",
    "    # Where 8 is the number of channels.\n",
    "    # Each channel represents a different object, which should have its own color.\n",
    "    # This is the order of the channels:\n",
    "    # channels.append(create_channel(state.a))  # agent\n",
    "    # channels.append(create_channel(state.m1))  # movable 1\n",
    "    # channels.append(create_channel(state.m2))  # movable 2\n",
    "    # channels.append(create_channel(state.m3))  # movable 3\n",
    "    # channels.append(create_channel(state.m4))  # movable 4\n",
    "    # channels.append(create_channel(puzzle.g1))  # goal 1\n",
    "    # channels.append(create_channel(puzzle.g2))  # goal 2\n",
    "    # channels.append(create_channel(puzzle.w))  # walls\n",
    "\n",
    "    # Movables that have associated goals should be given the \"movable_goal\" color,\n",
    "    # movables that do not should just be given the \"movable\" color.\n",
    "\n",
    "    # Convert to numpy for easier processing\n",
    "    obs_np = np.array(observation)\n",
    "    h, w = obs_np.shape[:2]\n",
    "\n",
    "    # Create RGB image initialized to white (empty spaces)\n",
    "    rgb_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    rgb_img.fill(255)  # white background\n",
    "\n",
    "    # Channel indices\n",
    "    AGENT_CH = 0\n",
    "    M1_CH, M2_CH, M3_CH, M4_CH = 1, 2, 3, 4\n",
    "    G1_CH, G2_CH = 5, 6\n",
    "    WALL_CH = 7\n",
    "\n",
    "    # Extract individual channels\n",
    "    agent = obs_np[:, :, AGENT_CH]\n",
    "    m1 = obs_np[:, :, M1_CH]\n",
    "    m2 = obs_np[:, :, M2_CH]\n",
    "    m3 = obs_np[:, :, M3_CH]\n",
    "    m4 = obs_np[:, :, M4_CH]\n",
    "    g1 = obs_np[:, :, G1_CH]\n",
    "    g2 = obs_np[:, :, G2_CH]\n",
    "    walls = obs_np[:, :, WALL_CH]\n",
    "\n",
    "    # Render walls first (bottom layer)\n",
    "    wall_mask = walls > 0\n",
    "    rgb_img[wall_mask] = symbol_to_rgb[4]  # black\n",
    "\n",
    "    # Render goals (light red for empty goals)\n",
    "    g1_mask = g1 > 0\n",
    "    g2_mask = g2 > 0\n",
    "    rgb_img[g1_mask] = hex_to_rgb(\"FF7F7F\")  # light red\n",
    "    rgb_img[g2_mask] = hex_to_rgb(\"FF7F7F\")  # light red\n",
    "\n",
    "    # Render movables with appropriate colors\n",
    "    # Goal-movable pairing is dynamic based on which goals exist:\n",
    "    # - If only g1 exists: m1 is the goal movable, m2/m3/m4 are regular movables\n",
    "    # - If both g1 and g2 exist: m1 and m2 are goal movables, m3/m4 are regular movables\n",
    "\n",
    "    # Check which goals exist\n",
    "    g1_exists = np.any(g1_mask)\n",
    "    g2_exists = np.any(g2_mask)\n",
    "\n",
    "    # m1: movable_goal color if on g1. g1 is guaranteed to always exist.\n",
    "    m1_mask = m1 > 0\n",
    "    rgb_img[m1_mask] = symbol_to_rgb[3]  # movable_goal (red)\n",
    "\n",
    "    # m2: movable_goal color if on g2 (when g2 exists), otherwise movable color\n",
    "    m2_mask = m2 > 0\n",
    "    if g2_exists:\n",
    "        rgb_img[m2_mask] = symbol_to_rgb[3]  # movable_goal (red)\n",
    "    else:\n",
    "        rgb_img[m2_mask] = symbol_to_rgb[2]  # movable (blue)\n",
    "\n",
    "    # m3 and m4: always regular movable color (no associated goals)\n",
    "    m3_mask = m3 > 0\n",
    "    m4_mask = m4 > 0\n",
    "    rgb_img[m3_mask] = symbol_to_rgb[2]  # movable (blue)\n",
    "    rgb_img[m4_mask] = symbol_to_rgb[2]  # movable (blue)\n",
    "\n",
    "    # Render agent on top\n",
    "    agent_mask = agent > 0\n",
    "    rgb_img[agent_mask] = symbol_to_rgb[1]  # agent (green)\n",
    "\n",
    "    # Upscale for better visibility (64x64 pixels per tile)\n",
    "    upscaled_img = np.kron(rgb_img, np.ones((64, 64, 1), dtype=np.uint8))\n",
    "\n",
    "    return upscaled_img\n",
    "\n",
    "\n",
    "def load_checkpoint_params(run_id):\n",
    "    api = wandb.Api()\n",
    "\n",
    "    # Construct the full artifact path\n",
    "    artifact_path = f\"kimyoungjin-nus/PushWorld/model-checkpoint-{run_id}:latest\"\n",
    "\n",
    "    # Fetch the artifact object directly\n",
    "    artifact = api.artifact(artifact_path)\n",
    "\n",
    "    # Download its contents\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "    checkpoint = orbax_checkpointer.restore(artifact_dir)\n",
    "\n",
    "    print(f\"Successfully loaded checkpoint at {artifact_dir}\")\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d74dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_meta(train_info, config, puzzles, video_name, eval_seed):\n",
    "    META_EPISODES = 10\n",
    "    # We're only going to sample from test anyway\n",
    "    benchmark = BenchmarkAll(\n",
    "        train_puzzles=puzzles,\n",
    "        test_puzzles=puzzles,\n",
    "    )\n",
    "\n",
    "    # setup environment\n",
    "    env = MetaTaskPushWorldEnvironmentAll()\n",
    "    env = GymAutoResetWrapper(env)\n",
    "    env_params = env.default_params()\n",
    "\n",
    "    rng = jax.random.key(eval_seed)\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    puzzle = benchmark.sample_puzzle(_rng, \"test\")\n",
    "    env_params = env_params.replace(puzzle=puzzle)\n",
    "\n",
    "    # you can use train_state from the final state also\n",
    "    # we just demo here how to do it if you loaded params from the checkpoint\n",
    "    params = train_info[\"state\"].params\n",
    "    model = ActorCriticRNN(\n",
    "        num_actions=env.num_actions(env_params),\n",
    "        action_emb_dim=config.action_emb_dim,\n",
    "        rnn_hidden_dim=config.rnn_hidden_dim,\n",
    "        rnn_num_layers=config.rnn_num_layers,\n",
    "        head_hidden_dim=config.head_hidden_dim,\n",
    "        img_obs=config.img_obs,\n",
    "    )\n",
    "\n",
    "    # jitting all functions\n",
    "    apply_fn, reset_fn, step_fn = jax.jit(model.apply), jax.jit(env.reset), jax.jit(env.step)\n",
    "\n",
    "    # for logging\n",
    "    total_reward, num_episodes = 0, 0\n",
    "    rendered_imgs = []\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    # initial inputs\n",
    "    hidden = model.initialize_carry(1)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    prev_action = jnp.asarray(0)\n",
    "\n",
    "    timestep = reset_fn(env_params, _rng)\n",
    "    rendered_imgs.append(text_to_rgb_all(timestep.observation))\n",
    "\n",
    "    while num_episodes < META_EPISODES:\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hidden = apply_fn(\n",
    "            params,\n",
    "            {\n",
    "                \"obs\": timestep.observation[None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hidden,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "\n",
    "        timestep = step_fn(env_params, timestep, action)\n",
    "        prev_action = action\n",
    "        prev_reward = timestep.reward\n",
    "\n",
    "        total_reward += timestep.reward.item()\n",
    "        num_episodes += int(timestep.last().item())\n",
    "        rendered_imgs.append(text_to_rgb_all(timestep.observation))\n",
    "\n",
    "    imageio.mimsave(f\"{video_name}.mp4\", rendered_imgs, fps=16, format=\"mp4\")\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f2d10",
   "metadata": {},
   "source": [
    "## Video Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tracking per-episode statistics during meta-RL evaluation\n",
    "class VideoMetaRolloutStats(struct.PyTreeNode):\n",
    "    episode_rewards: jax.Array  # Shape: [max_episodes] - reward for each episode\n",
    "    episode_lengths: jax.Array  # Shape: [max_episodes] - length of each episode\n",
    "    episode_solved: jax.Array  # Shape: [max_episodes] - whether each episode was solved\n",
    "    total_reward: jax.Array  # Scalar - total reward across all episodes\n",
    "    num_episodes_completed: jax.Array  # Scalar - number of episodes actually completed\n",
    "    video_frames: (\n",
    "        jax.Array\n",
    "    )  # Number of frames should be num_consecutive_episodes * 100, since each episode is 100 steps.\n",
    "    # Therefore shape should be [max_episodes * 100, H, W, 3]\n",
    "\n",
    "\n",
    "COLOR_MAP = jnp.array(\n",
    "    [\n",
    "        [255, 255, 255],  # 0: White (background)\n",
    "        [0, 255, 0],  # 1: Green (agent)\n",
    "        [0, 0, 255],  # 2: Blue (movable)\n",
    "        [255, 0, 0],  # 3: Red (movable_goal)\n",
    "        [0, 0, 0],  # 4: Black (wall)\n",
    "        [255, 127, 127],  # 5: Light Red (empty goal)\n",
    "        [255, 0, 0],  # 6: Red (filled goal - same as movable_goal)\n",
    "    ],\n",
    "    dtype=jnp.uint8,\n",
    ")\n",
    "\n",
    "\n",
    "def obs_to_rgb(observation: jax.Array) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Renders a grid world observation into an RGB image using JAX operations.\n",
    "\n",
    "    Args:\n",
    "        observation: A jax.Array of shape (H, W, 8) representing the grid state.\n",
    "\n",
    "    Returns:\n",
    "        A jax.Array of shape (H*64, W*64, 3) representing the upscaled RGB image.\n",
    "    \"\"\"\n",
    "    h, w, _ = observation.shape\n",
    "\n",
    "    # Channel indices for clarity\n",
    "    AGENT_CH = 0\n",
    "    M1_CH, M2_CH, M3_CH, M4_CH = 1, 2, 3, 4\n",
    "    G1_CH, G2_CH = 5, 6\n",
    "    WALL_CH = 7\n",
    "\n",
    "    # --- 1. Create a base canvas ---\n",
    "    # Start with a white background.\n",
    "    # The shape is (H, W), and each element is an index into the COLOR_MAP.\n",
    "    canvas = jnp.zeros((h, w), dtype=jnp.int32)\n",
    "\n",
    "    # --- 2. Layer the static objects ---\n",
    "    # jnp.where(condition, value_if_true, value_if_false)\n",
    "    # Layer walls\n",
    "    wall_mask = observation[:, :, WALL_CH] > 0\n",
    "    canvas = jnp.where(wall_mask, 4, canvas)  # 4 is the index for WALL color\n",
    "\n",
    "    # Layer goals (as empty goals)\n",
    "    g1_mask = observation[:, :, G1_CH] > 0\n",
    "    g2_mask = observation[:, :, G2_CH] > 0\n",
    "    canvas = jnp.where(g1_mask, 5, canvas)  # 5 is the index for GOAL_EMPTY color\n",
    "    canvas = jnp.where(g2_mask, 5, canvas)\n",
    "\n",
    "    # --- 3. Determine movable colors based on goal existence ---\n",
    "    # This avoids Python-level if/else statements.\n",
    "    g1_exists = jnp.any(g1_mask)\n",
    "    g2_exists = jnp.any(g2_mask)\n",
    "\n",
    "    # Color for m1 is always 'movable_goal' as g1 always exists.\n",
    "    m1_color_idx = 3  # movable_goal (red)\n",
    "\n",
    "    # Color for m2 depends on whether g2 exists.\n",
    "    # jnp.where works on scalars too, making it great for conditional logic.\n",
    "    m2_color_idx = jnp.where(\n",
    "        g2_exists,\n",
    "        3,  # movable_goal (red) if g2 exists\n",
    "        2,\n",
    "    )  # movable (blue) if g2 does not exist\n",
    "\n",
    "    # m3 and m4 are always regular 'movable'.\n",
    "    m3_color_idx = 2  # movable (blue)\n",
    "    m4_color_idx = 2  # movable (blue)\n",
    "\n",
    "    # --- 4. Layer the movables ---\n",
    "    m1_mask = observation[:, :, M1_CH] > 0\n",
    "    m2_mask = observation[:, :, M2_CH] > 0\n",
    "    m3_mask = observation[:, :, M3_CH] > 0\n",
    "    m4_mask = observation[:, :, M4_CH] > 0\n",
    "\n",
    "    canvas = jnp.where(m1_mask, m1_color_idx, canvas)\n",
    "    canvas = jnp.where(m2_mask, m2_color_idx, canvas)\n",
    "    canvas = jnp.where(m3_mask, m3_color_idx, canvas)\n",
    "    canvas = jnp.where(m4_mask, m4_color_idx, canvas)\n",
    "\n",
    "    # --- 5. Handle filled goals ---\n",
    "    # If a movable is on a goal, the color should be the 'filled goal' color.\n",
    "    # This is an overlay operation.\n",
    "    m1_on_g1 = m1_mask & g1_mask\n",
    "    m2_on_g2 = m2_mask & g2_mask\n",
    "    canvas = jnp.where(m1_on_g1, 6, canvas)  # 6 is the index for GOAL_FILLED\n",
    "    canvas = jnp.where(g2_exists, jnp.where(m2_on_g2, 6, canvas), canvas)\n",
    "\n",
    "    # --- 6. Layer the agent on top ---\n",
    "    agent_mask = observation[:, :, AGENT_CH] > 0\n",
    "    canvas = jnp.where(agent_mask, 1, canvas)  # 1 is the index for AGENT color\n",
    "\n",
    "    # --- 7. Convert the canvas of indices to an RGB image ---\n",
    "    # This is a powerful indexing operation in JAX.\n",
    "    rgb_img = COLOR_MAP[canvas]\n",
    "\n",
    "    # --- 8. Upscale for better visibility ---\n",
    "    # upscaled_img = jnp.kron(rgb_img, jnp.ones((64, 64, 1), dtype=jnp.uint8))\n",
    "\n",
    "    return rgb_img\n",
    "\n",
    "\n",
    "def meta_rollout_video(\n",
    "    rng: jax.Array,\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    eval_puzzle: PushWorldPuzzleAll,\n",
    "    params,\n",
    "    network: ActorCriticRNN,\n",
    "    init_hstate: jax.Array,\n",
    "    num_consecutive_episodes: int = 1,\n",
    ") -> VideoMetaRolloutStats:\n",
    "    \"\"\"Rollout that tracks statistics for each individual episode.\"\"\"\n",
    "\n",
    "    def _reset_env_fn(carry_components):\n",
    "        \"\"\"Function to execute when the episode has ended.\"\"\"\n",
    "        timestep, env_params, rng = carry_components\n",
    "        key, _ = jax.random.split(rng)\n",
    "        reset_timestep = env.reset(env_params, key)\n",
    "\n",
    "        # Return a new timestep with the reset state and observation\n",
    "        return timestep.replace(\n",
    "            state=reset_timestep.state,\n",
    "            observation=reset_timestep.observation,\n",
    "        )\n",
    "\n",
    "    def _identity_fn(carry_components):\n",
    "        \"\"\"Function to execute when the episode has not ended.\"\"\"\n",
    "        timestep, _, _ = carry_components\n",
    "        # Return the timestep unchanged\n",
    "        return timestep\n",
    "\n",
    "    def _cond_fn(carry):\n",
    "        (\n",
    "            rng,\n",
    "            stats,\n",
    "            timestep,\n",
    "            prev_action,\n",
    "            prev_reward,\n",
    "            hstate,\n",
    "            current_episode_reward,\n",
    "            current_episode_length,\n",
    "            step_num,\n",
    "        ) = carry\n",
    "        return jnp.less(stats.num_episodes_completed, num_consecutive_episodes)\n",
    "\n",
    "    def _body_fn(carry):\n",
    "        (\n",
    "            rng,\n",
    "            stats,\n",
    "            timestep,\n",
    "            prev_action,\n",
    "            prev_reward,\n",
    "            hstate,\n",
    "            current_episode_reward,\n",
    "            current_episode_length,\n",
    "            step_num,\n",
    "        ) = carry\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hstate = network.apply(\n",
    "            params,\n",
    "            {\n",
    "                # We add single channel dimension to end of obs_img\n",
    "                \"obs\": timestep.observation[None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "        timestep = env.step(env_params, timestep, action)\n",
    "\n",
    "        # Update current episode accumulators\n",
    "        current_episode_reward = current_episode_reward + timestep.reward\n",
    "        current_episode_length = current_episode_length + 1\n",
    "\n",
    "        # Check if episode ended\n",
    "        episode_ended = timestep.last()\n",
    "        solved_flag = ((timestep.reward == SUCCESS_REWARD) & (episode_ended == 1)).astype(jnp.int32)\n",
    "\n",
    "        # When episode ends, store the episode stats\n",
    "        episode_idx = stats.num_episodes_completed\n",
    "        new_episode_rewards = stats.episode_rewards.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, current_episode_reward, stats.episode_rewards[episode_idx])\n",
    "        )\n",
    "        new_episode_lengths = stats.episode_lengths.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, current_episode_length, stats.episode_lengths[episode_idx])\n",
    "        )\n",
    "        new_episode_solved = stats.episode_solved.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, solved_flag, stats.episode_solved[episode_idx])\n",
    "        )\n",
    "\n",
    "        # Update stats\n",
    "        stats = stats.replace(\n",
    "            episode_rewards=new_episode_rewards,\n",
    "            episode_lengths=new_episode_lengths,\n",
    "            episode_solved=new_episode_solved,\n",
    "            total_reward=stats.total_reward + timestep.reward,\n",
    "            num_episodes_completed=stats.num_episodes_completed + episode_ended,\n",
    "            video_frames=stats.video_frames.at[step_num].set(obs_to_rgb(timestep.observation)),\n",
    "        )\n",
    "\n",
    "        # Reset episode accumulators when episode ends\n",
    "        current_episode_reward = jnp.where(episode_ended, 0.0, current_episode_reward)\n",
    "        current_episode_length = jnp.where(episode_ended, 0, current_episode_length)\n",
    "\n",
    "        operands = (timestep, env_params, rng)\n",
    "        timestep = jax.lax.cond(\n",
    "            episode_ended,\n",
    "            _reset_env_fn,  # The function to run if True\n",
    "            _identity_fn,  # The function to run if False\n",
    "            operands,  # The arguments passed to either function\n",
    "        )\n",
    "\n",
    "        carry = (\n",
    "            rng,\n",
    "            stats,\n",
    "            timestep,\n",
    "            action,\n",
    "            timestep.reward,\n",
    "            hstate,\n",
    "            current_episode_reward,\n",
    "            current_episode_length,\n",
    "            step_num + 1,\n",
    "        )\n",
    "        return carry\n",
    "\n",
    "    # Initialize episode tracking arrays\n",
    "    episode_rewards = jnp.zeros(num_consecutive_episodes)\n",
    "    episode_lengths = jnp.zeros(num_consecutive_episodes, dtype=jnp.int32)\n",
    "    episode_solved = jnp.zeros(num_consecutive_episodes, dtype=jnp.int32)\n",
    "\n",
    "    initial_stats = VideoMetaRolloutStats(\n",
    "        episode_rewards=episode_rewards,\n",
    "        episode_lengths=episode_lengths,\n",
    "        episode_solved=episode_solved,\n",
    "        total_reward=jnp.asarray(0.0),\n",
    "        num_episodes_completed=jnp.asarray(0),\n",
    "        video_frames=jnp.zeros((num_consecutive_episodes * 100, LEVEL0_ALL_SIZE, LEVEL0_ALL_SIZE, 3), dtype=jnp.uint8),\n",
    "    )\n",
    "\n",
    "    env_params = env_params.replace(puzzle=eval_puzzle)\n",
    "    timestep = env.eval_reset(env_params, rng)\n",
    "    prev_action = jnp.asarray(0)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    current_episode_reward = jnp.asarray(0.0)\n",
    "    current_episode_length = jnp.asarray(0)\n",
    "\n",
    "    init_carry = (\n",
    "        rng,\n",
    "        initial_stats,\n",
    "        timestep,\n",
    "        prev_action,\n",
    "        prev_reward,\n",
    "        init_hstate,\n",
    "        current_episode_reward,\n",
    "        current_episode_length,\n",
    "        jnp.asarray(0),\n",
    "    )\n",
    "\n",
    "    final_carry = jax.lax.while_loop(_cond_fn, _body_fn, init_val=init_carry)\n",
    "    return final_carry[1]\n",
    "\n",
    "\n",
    "# Set up environment and run rollouts to get videos\n",
    "def get_videos_for_all(config, model_params):\n",
    "    # Get model params from W&B\n",
    "    env = MetaTaskPushWorldEnvironmentAll()\n",
    "    env_params = env.default_params()\n",
    "    # We want to include the final goal-reaching frame\n",
    "    # env = GymAutoResetWrapper(env)\n",
    "\n",
    "    benchmark = pushworld.load_all_benchmark(config.benchmark_id)\n",
    "\n",
    "    puzzle_rng = jax.random.key(config.puzzle_seed)\n",
    "    train_rng, test_rng = jax.random.split(puzzle_rng)\n",
    "\n",
    "    if config.num_train is not None:\n",
    "        assert config.num_train <= benchmark.num_train_puzzles(), (\n",
    "            \"num_train is larger than num train available in benchmark\"\n",
    "        )\n",
    "        perm = jax.random.permutation(train_rng, benchmark.num_train_puzzles())\n",
    "        idxs = perm[: config.num_train]\n",
    "        benchmark = benchmark.replace(train_puzzles=benchmark.train_puzzles[idxs])\n",
    "    else:\n",
    "        config.num_train = benchmark.num_train_puzzles()\n",
    "\n",
    "    if config.num_test is not None:\n",
    "        assert config.num_test <= benchmark.num_test_puzzles(), (\n",
    "            \"num_test is larger than num test available in benchmark\"\n",
    "        )\n",
    "        perm = jax.random.permutation(test_rng, benchmark.num_test_puzzles())\n",
    "        idxs = perm[: config.num_test]\n",
    "        benchmark = benchmark.replace(test_puzzles=benchmark.test_puzzles[idxs])\n",
    "    else:\n",
    "        config.num_test = benchmark.num_test_puzzles()\n",
    "\n",
    "    if config.train_test_same:\n",
    "        benchmark = benchmark.replace(test_puzzles=benchmark.train_puzzles)\n",
    "        config.num_test = config.num_train\n",
    "\n",
    "    network = ActorCriticRNN(\n",
    "        num_actions=env.num_actions(env_params),\n",
    "        obs_emb_dim=config.obs_emb_dim,\n",
    "        action_emb_dim=config.action_emb_dim,\n",
    "        rnn_hidden_dim=config.rnn_hidden_dim,\n",
    "        rnn_num_layers=config.rnn_num_layers,\n",
    "        head_hidden_dim=config.head_hidden_dim,\n",
    "        img_obs=config.img_obs,\n",
    "        dtype=jnp.bfloat16 if config.enable_bf16 else None,\n",
    "    )\n",
    "\n",
    "    init_hstate = network.initialize_carry(batch_size=config.num_envs_per_device)\n",
    "    eval_hstate = init_hstate[0][None]\n",
    "\n",
    "    eval_reset_rng = jax.random.key(config.eval_seed)\n",
    "    eval_test_rng, eval_train_rng = jax.random.split(eval_reset_rng)\n",
    "\n",
    "    eval_test_reset_rng = jax.random.split(eval_test_rng, num=config.num_test)\n",
    "    eval_test_puzzles = benchmark.get_test_puzzles()\n",
    "    eval_test_stats = jax.vmap(meta_rollout_video, in_axes=(0, None, None, 0, None, None, None, None))(\n",
    "        eval_test_reset_rng,\n",
    "        env,\n",
    "        env_params,\n",
    "        eval_test_puzzles,\n",
    "        model_params,\n",
    "        network,\n",
    "        eval_hstate,\n",
    "        config.eval_num_episodes,\n",
    "    )\n",
    "    # eval_test_stats = jax.lax.pmean(eval_test_stats, axis_name=\"devices\")\n",
    "\n",
    "    # Eval on train set\n",
    "    eval_train_reset_rng = jax.random.split(eval_train_rng, num=config.num_train)\n",
    "    eval_train_puzzles = benchmark.get_train_puzzles()\n",
    "    eval_train_stats = jax.vmap(meta_rollout_video, in_axes=(0, None, None, 0, None, None, None, None))(\n",
    "        eval_train_reset_rng,\n",
    "        env,\n",
    "        env_params,\n",
    "        eval_train_puzzles,\n",
    "        model_params,\n",
    "        network,\n",
    "        eval_hstate,\n",
    "        config.eval_num_episodes,\n",
    "    )\n",
    "    # eval_train_stats = jax.lax.pmean(eval_train_stats, axis_name=\"devices\")\n",
    "\n",
    "    return eval_test_stats, eval_train_stats\n",
    "\n",
    "\n",
    "def upscale_and_save(videos, ids, save_path, name, batch_size=10):\n",
    "    \"\"\"\n",
    "    Upscales and saves a collection of videos to disk in batches.\n",
    "\n",
    "    This function processes videos in small batches to avoid high memory usage\n",
    "    when dealing with large datasets. Each frame is upscaled, and the resulting\n",
    "    video is saved as an MP4.\n",
    "\n",
    "    Args:\n",
    "        videos (jax.Array): A JAX array of video frames.\n",
    "        ids (list | jax.Array): A list or array of identifiers for each video, used for naming files.\n",
    "        save_path (str): The directory where the output MP4 files will be saved.\n",
    "        name (str): A prefix used for the output filenames (e.g., 'train_success').\n",
    "        batch_size (int): The number of videos to process in each batch.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # 1. Upscale\n",
    "    def upscale_frame(frame):\n",
    "        \"\"\"Upscales a single image frame.\"\"\"\n",
    "        scale_factor = 16\n",
    "        upscale_matrix = jnp.ones((scale_factor, scale_factor, 1), dtype=jnp.uint8)\n",
    "        return jnp.kron(frame, upscale_matrix)\n",
    "\n",
    "    vmapped_upscaler = jax.jit(jax.vmap(upscale_frame))\n",
    "\n",
    "    # --- 2. The Batching Loop ---\n",
    "    num_videos = len(ids)\n",
    "    for i in range(0, num_videos, batch_size):\n",
    "        # Determine the start and end index for the current batch\n",
    "        start_idx = i\n",
    "        end_idx = min(i + batch_size, num_videos)\n",
    "\n",
    "        print(f\"Processing batch {i // batch_size + 1}: Videos {start_idx} to {end_idx - 1}\")\n",
    "\n",
    "        # Get the current batch of videos and IDs\n",
    "        video_batch = videos[start_idx:end_idx]\n",
    "        id_batch = ids[start_idx:end_idx]\n",
    "\n",
    "        # --- 3. Run JAX computation ONLY on the small batch ---\n",
    "\n",
    "        # This now only allocates memory for one upscaled batch, not the whole dataset\n",
    "        upscaled_batch = vmapped_upscaler(video_batch)\n",
    "\n",
    "        # Block until computation is finished before saving\n",
    "        upscaled_batch.block_until_ready()\n",
    "\n",
    "        # --- 4. Save the processed batch to disk ---\n",
    "        for j, video_id in enumerate(id_batch):\n",
    "            # Get the single video from the processed batch\n",
    "            video_data = upscaled_batch[j]\n",
    "            file_path = os.path.join(save_path, f\"{name}_{video_id}.mp4\")\n",
    "            imageio.mimsave(file_path, video_data, fps=16, format=\"mp4\")\n",
    "\n",
    "    print(\"All batches processed and saved successfully.\")\n",
    "\n",
    "\n",
    "def upscale_and_save_videos(\n",
    "    train_stats,\n",
    "    test_stats,\n",
    "    video_save_root,\n",
    "    max_train_success=100,\n",
    "    max_train_fail=100,\n",
    "    max_test_success=None,\n",
    "    max_test_fail=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sorts videos from training and testing sets by outcome and saves them.\n",
    "\n",
    "    This function filters videos into 'succeeded' and 'failed' categories based on\n",
    "    the final solved status in the provided statistics. It then calls a utility\n",
    "    to process and save them into separate subdirectories (e.g., 'train/succeeded',\n",
    "    'test/failed').\n",
    "\n",
    "    Args:\n",
    "        train_stats: An object containing `video_frames` and `episode_solved`\n",
    "                     data for the training set.\n",
    "        test_stats: An object containing `video_frames` and `episode_solved`\n",
    "                    data for the testing set.\n",
    "        video_save_root (str): The root directory where the categorized video\n",
    "                         folders will be created.\n",
    "    \"\"\"\n",
    "    train_succeed_idx = jnp.where(train_stats.episode_solved[:, -1] == 1)[0]\n",
    "    train_fail_idx = jnp.where(train_stats.episode_solved[:, -1] == 0)[0]\n",
    "    test_succeed_idx = jnp.where(test_stats.episode_solved[:, -1] == 1)[0]\n",
    "    test_fail_idx = jnp.where(test_stats.episode_solved[:, -1] == 0)[0]\n",
    "\n",
    "    upscale_and_save(\n",
    "        train_stats.video_frames[train_succeed_idx][:max_train_success],\n",
    "        train_succeed_idx[:max_train_success],\n",
    "        os.path.join(video_save_root, \"train\", \"succeeded\"),\n",
    "        \"train_success\",\n",
    "    )\n",
    "\n",
    "    upscale_and_save(\n",
    "        train_stats.video_frames[train_fail_idx][:max_train_fail],\n",
    "        train_fail_idx[:max_train_fail],\n",
    "        os.path.join(video_save_root, \"train\", \"failed\"),\n",
    "        \"train_fail\",\n",
    "    )\n",
    "\n",
    "    upscale_and_save(\n",
    "        test_stats.video_frames[test_succeed_idx][:max_test_success],\n",
    "        test_succeed_idx[:max_test_success],\n",
    "        os.path.join(video_save_root, \"test\", \"succeeded\"),\n",
    "        \"test_success\",\n",
    "    )\n",
    "\n",
    "    upscale_and_save(\n",
    "        test_stats.video_frames[test_fail_idx][:max_test_fail],\n",
    "        test_fail_idx[:max_test_fail],\n",
    "        os.path.join(video_save_root, \"test\", \"failed\"),\n",
    "        \"test_fail\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488192a8",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\" # fill in\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    benchmark_id=\"level0_transformed_all\",\n",
    "    total_timesteps=1000_000_000,\n",
    "    num_envs=8192,\n",
    "    num_steps_per_env=500,\n",
    "    num_steps_per_update=500,\n",
    "    train_test_same=False,\n",
    "    num_train=2000,\n",
    "    num_test=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info, elapsed_time = train(config)\n",
    "processing(config, train_info, elapsed_time)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
