{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c94e210",
   "metadata": {},
   "source": [
    "# Meta-task PushWorld Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install \"xminigrid[baselines] @ git+https://github.com/jugheadjones10/xland-minigrid.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time #noqa\n",
    "import os #noqa\n",
    "import math # noqa\n",
    "from typing import TypedDict, Optional, Literal #noqa\n",
    "import numpy as np #noqa\n",
    "import importlib #noqa\n",
    "import os #noqa\n",
    "\n",
    "import jax #noqa\n",
    "import jax.numpy as jnp #noqa\n",
    "import jax.tree_util as jtu #noqa\n",
    "import flax #noqa\n",
    "import flax.linen as nn #noqa\n",
    "from flax.training import orbax_utils #noqa\n",
    "import distrax #noqa\n",
    "import orbax #noqa\n",
    "import optax #noqa\n",
    "import imageio #noqa\n",
    "import wandb #noqa\n",
    "import matplotlib.pyplot as plt #noqa\n",
    "\n",
    "from flax import struct #noqa\n",
    "from flax.typing import Dtype #noqa\n",
    "from flax.linen.dtypes import promote_dtype #noqa\n",
    "from flax.linen.initializers import glorot_normal, orthogonal, zeros_init #noqa\n",
    "from flax.training.train_state import TrainState #noqa\n",
    "from flax.jax_utils import replicate, unreplicate #noqa\n",
    "from dataclasses import asdict, dataclass #noqa\n",
    "from functools import partial #noqa\n",
    "\n",
    "import xminigrid.envs.pushworld as pushworld\n",
    "from xminigrid.envs.pushworld.benchmarks import Benchmark, BenchmarkAll\n",
    "from xminigrid.envs.pushworld.constants import Tiles, NUM_TILES, SUCCESS_REWARD\n",
    "from xminigrid.envs.pushworld.environment import Environment, EnvParams, EnvParamsT\n",
    "from xminigrid.envs.pushworld.envs.single_task_pushworld import SingleTaskPushWorldEnvironment, SingleTaskPushWorldEnvParams\n",
    "from xminigrid.envs.pushworld.envs.meta_task_pushworld import MetaTaskPushWorldEnvironment\n",
    "# Import level 0 \"all\" environments\n",
    "from xminigrid.envs.pushworld.envs.single_task_all_pushworld import SingleTaskPushWorldEnvironmentAll \n",
    "from xminigrid.envs.pushworld.envs.meta_task_all_pushworld import MetaTaskPushWorldEnvironmentAll\n",
    "from xminigrid.envs.pushworld.scripts.upload import encode_puzzle\n",
    "from xminigrid.envs.pushworld.wrappers import GoalObservationWrapper, GymAutoResetWrapper, Wrapper\n",
    "from xminigrid.envs.pushworld.types import State, TimeStep, StepType, EnvCarry, PushWorldPuzzle, PushWorldPuzzleAll\n",
    "from xminigrid.envs.pushworld.grid import get_obs_from_puzzle\n",
    "from IPython.display import Video, HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d189d",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model adapted from minigrid baselines:\n",
    "# https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    hidden_dim: int\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, xs, init_state):\n",
    "        seq_len, input_dim = xs.shape\n",
    "        # this init might not be optimal, for example bias for reset gate should be -1 (for now ok)\n",
    "        Wi = self.param(\"Wi\", glorot_normal(in_axis=1, out_axis=0), (self.hidden_dim * 3, input_dim), self.param_dtype)\n",
    "        Wh = self.param(\"Wh\", orthogonal(column_axis=0), (self.hidden_dim * 3, self.hidden_dim), self.param_dtype)\n",
    "        bi = self.param(\"bi\", zeros_init(), (self.hidden_dim * 3,), self.param_dtype)\n",
    "        bn = self.param(\"bn\", zeros_init(), (self.hidden_dim,), self.param_dtype)\n",
    "\n",
    "        def _step_fn(h, x):\n",
    "            igates = jnp.split(Wi @ x + bi, 3)\n",
    "            hgates = jnp.split(Wh @ h, 3)\n",
    "\n",
    "            reset = nn.sigmoid(igates[0] + hgates[0])\n",
    "            update = nn.sigmoid(igates[1] + hgates[1])\n",
    "            new = nn.tanh(igates[2] + reset * (hgates[2] + bn))\n",
    "            next_h = (1 - update) * new + update * h\n",
    "\n",
    "            return next_h, next_h\n",
    "\n",
    "        # cast to the computation dtype\n",
    "        xs, init_state, Wi, Wh, bi, bn = promote_dtype(xs, init_state, Wi, Wh, bi, bn, dtype=self.dtype)\n",
    "\n",
    "        last_state, all_states = jax.lax.scan(_step_fn, init=init_state, xs=xs)\n",
    "        return all_states, last_state\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    hidden_dim: int\n",
    "    num_layers: int\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, xs, init_state):\n",
    "        # xs: [seq_len, input_dim]\n",
    "        # init_state: [num_layers, hidden_dim]\n",
    "        outs, states = [], []\n",
    "        for layer in range(self.num_layers):\n",
    "            xs, state = GRU(self.hidden_dim, self.dtype, self.param_dtype)(xs, init_state[layer])\n",
    "            outs.append(xs)\n",
    "            states.append(state)\n",
    "\n",
    "        # sum outputs from all layers, kinda like in ResNet\n",
    "        return jnp.array(outs).sum(0), jnp.array(states)\n",
    "\n",
    "\n",
    "BatchedRNNModel = flax.linen.vmap(\n",
    "    RNNModel, variable_axes={\"params\": None}, split_rngs={\"params\": False}, axis_name=\"batch\"\n",
    ")\n",
    "\n",
    "\n",
    "class EmbeddingEncoder(nn.Module):\n",
    "    emb_dim: int = 16\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, img):\n",
    "        entity_emb = nn.Embed(NUM_TILES, self.emb_dim, self.dtype, self.param_dtype)\n",
    "\n",
    "        # [..., channels]\n",
    "        img_emb = entity_emb(img[..., 0])\n",
    "        return img_emb\n",
    "\n",
    "\n",
    "class ActorCriticInput(TypedDict):\n",
    "    obs_img: jax.Array\n",
    "    obs_goal: jax.Array\n",
    "    prev_action: jax.Array\n",
    "    prev_reward: jax.Array\n",
    "\n",
    "\n",
    "class ActorCriticRNN(nn.Module):\n",
    "    num_actions: int\n",
    "    obs_emb_dim: int = 16\n",
    "    action_emb_dim: int = 16\n",
    "    rnn_hidden_dim: int = 64\n",
    "    rnn_num_layers: int = 1\n",
    "    head_hidden_dim: int = 64\n",
    "    img_obs: bool = False\n",
    "    dtype: Optional[Dtype] = None\n",
    "    param_dtype: Dtype = jnp.float32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs: ActorCriticInput, hidden: jax.Array) -> tuple[distrax.Categorical, jax.Array, jax.Array]:\n",
    "        B, S = inputs[\"obs_img\"].shape[:2]\n",
    "\n",
    "        # encoder from https://github.com/lcswillems/rl-starter-files/blob/master/model.py\n",
    "        if self.img_obs:\n",
    "            img_encoder = nn.Sequential(\n",
    "                [\n",
    "                    nn.Conv(\n",
    "                        16,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (3, 3),\n",
    "                        strides=2,\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            img_encoder = nn.Sequential(\n",
    "                [\n",
    "                    # For small dims nn.Embed is extremely slow in bf16, so we leave everything in default dtypes\n",
    "                    EmbeddingEncoder(emb_dim=self.obs_emb_dim),\n",
    "                    nn.Conv(\n",
    "                        16,\n",
    "                        (2, 2),\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        32,\n",
    "                        (2, 2),\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                    nn.Conv(\n",
    "                        64,\n",
    "                        (2, 2),\n",
    "                        padding=\"VALID\",\n",
    "                        kernel_init=orthogonal(math.sqrt(2)),\n",
    "                        dtype=self.dtype,\n",
    "                        param_dtype=self.param_dtype,\n",
    "                    ),\n",
    "                    nn.relu,\n",
    "                ]\n",
    "            )\n",
    "        action_encoder = nn.Embed(self.num_actions, self.action_emb_dim)\n",
    "        goal_encoder = nn.Dense(self.action_emb_dim, dtype=self.dtype, param_dtype=self.param_dtype)\n",
    "\n",
    "        rnn_core = BatchedRNNModel(\n",
    "            self.rnn_hidden_dim, self.rnn_num_layers, dtype=self.dtype, param_dtype=self.param_dtype\n",
    "        )\n",
    "        actor = nn.Sequential(\n",
    "            [\n",
    "                nn.Dense(\n",
    "                    self.head_hidden_dim, kernel_init=orthogonal(2), dtype=self.dtype, param_dtype=self.param_dtype\n",
    "                ),\n",
    "                nn.tanh,\n",
    "                nn.Dense(\n",
    "                    self.num_actions, kernel_init=orthogonal(0.01), dtype=self.dtype, param_dtype=self.param_dtype\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        critic = nn.Sequential(\n",
    "            [\n",
    "                nn.Dense(\n",
    "                    self.head_hidden_dim, kernel_init=orthogonal(2), dtype=self.dtype, param_dtype=self.param_dtype\n",
    "                ),\n",
    "                nn.tanh,\n",
    "                nn.Dense(1, kernel_init=orthogonal(1.0), dtype=self.dtype, param_dtype=self.param_dtype),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # [batch_size, seq_len, ...]\n",
    "        obs_emb = img_encoder(inputs[\"obs_img\"].astype(jnp.int32)).reshape(B, S, -1)\n",
    "        goal_emb = goal_encoder(inputs[\"obs_goal\"])\n",
    "        act_emb = action_encoder(inputs[\"prev_action\"])\n",
    "\n",
    "        # [batch_size, seq_len, hidden_dim + 2 * act_emb_dim + 1]\n",
    "        out = jnp.concatenate([obs_emb, goal_emb, act_emb, inputs[\"prev_reward\"][..., None]], axis=-1)\n",
    "\n",
    "        # core networks\n",
    "        out, new_hidden = rnn_core(out, hidden)\n",
    "\n",
    "        # casting to full precision for the loss, as softmax/log_softmax\n",
    "        # (inside Categorical) is not stable in bf16\n",
    "        logits = actor(out).astype(jnp.float32)\n",
    "\n",
    "        dist = distrax.Categorical(logits=logits)\n",
    "        values = critic(out)\n",
    "\n",
    "        return dist, jnp.squeeze(values, axis=-1), new_hidden\n",
    "\n",
    "    def initialize_carry(self, batch_size):\n",
    "        return jnp.zeros((batch_size, self.rnn_num_layers, self.rnn_hidden_dim), dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299daaa1",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for PPO training and evaluation\n",
    "\n",
    "\n",
    "# Training stuff\n",
    "class Transition(struct.PyTreeNode):\n",
    "    done: jax.Array\n",
    "    action: jax.Array\n",
    "    value: jax.Array\n",
    "    reward: jax.Array\n",
    "    log_prob: jax.Array\n",
    "    # for obs\n",
    "    obs: jax.Array\n",
    "    goal: jax.Array\n",
    "    # for rnn policy\n",
    "    prev_action: jax.Array\n",
    "    prev_reward: jax.Array\n",
    "\n",
    "\n",
    "def calculate_gae(\n",
    "    transitions: Transition,\n",
    "    last_val: jax.Array,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> tuple[jax.Array, jax.Array]:\n",
    "    # single iteration for the loop\n",
    "    def _get_advantages(gae_and_next_value, transition):\n",
    "        gae, next_value = gae_and_next_value\n",
    "        delta = transition.reward + gamma * next_value * (1 - transition.done) - transition.value\n",
    "        gae = delta + gamma * gae_lambda * (1 - transition.done) * gae\n",
    "        return (gae, transition.value), gae\n",
    "\n",
    "    _, advantages = jax.lax.scan(\n",
    "        _get_advantages,\n",
    "        (jnp.zeros_like(last_val), last_val),\n",
    "        transitions,\n",
    "        reverse=True,\n",
    "    )\n",
    "    # advantages and values (Q)\n",
    "    return advantages, advantages + transitions.value\n",
    "\n",
    "\n",
    "def ppo_update_networks(\n",
    "    train_state: TrainState,\n",
    "    transitions: Transition,\n",
    "    init_hstate: jax.Array,\n",
    "    advantages: jax.Array,\n",
    "    targets: jax.Array,\n",
    "    clip_eps: float,\n",
    "    vf_coef: float,\n",
    "    ent_coef: float,\n",
    "):\n",
    "    # NORMALIZE ADVANTAGES\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    def _loss_fn(params):\n",
    "        # RERUN NETWORK\n",
    "        dist, value, _ = train_state.apply_fn(\n",
    "            params,\n",
    "            {\n",
    "                # [batch_size, seq_len, ...]\n",
    "                \"obs_img\": transitions.obs,\n",
    "                \"obs_goal\": transitions.goal,\n",
    "                \"prev_action\": transitions.prev_action,\n",
    "                \"prev_reward\": transitions.prev_reward,\n",
    "            },\n",
    "            init_hstate,\n",
    "        )\n",
    "        log_prob = dist.log_prob(transitions.action)\n",
    "\n",
    "        # CALCULATE VALUE LOSS\n",
    "        value_pred_clipped = transitions.value + (value - transitions.value).clip(-clip_eps, clip_eps)\n",
    "        value_loss = jnp.square(value - targets)\n",
    "        value_loss_clipped = jnp.square(value_pred_clipped - targets)\n",
    "        value_loss = 0.5 * jnp.maximum(value_loss, value_loss_clipped).mean()\n",
    "\n",
    "        # TODO: ablate this!\n",
    "        # value_loss = jnp.square(value - targets).mean()\n",
    "\n",
    "        # CALCULATE ACTOR LOSS\n",
    "        ratio = jnp.exp(log_prob - transitions.log_prob)\n",
    "        actor_loss1 = advantages * ratio\n",
    "        actor_loss2 = advantages * jnp.clip(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "        actor_loss = -jnp.minimum(actor_loss1, actor_loss2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        total_loss = actor_loss + vf_coef * value_loss - ent_coef * entropy\n",
    "        return total_loss, (value_loss, actor_loss, entropy)\n",
    "\n",
    "    (loss, (vloss, aloss, entropy)), grads = jax.value_and_grad(_loss_fn, has_aux=True)(train_state.params)\n",
    "    (loss, vloss, aloss, entropy, grads) = jax.lax.pmean((loss, vloss, aloss, entropy, grads), axis_name=\"devices\")\n",
    "    train_state = train_state.apply_gradients(grads=grads)\n",
    "    update_info = {\n",
    "        \"total_loss\": loss,\n",
    "        \"value_loss\": vloss,\n",
    "        \"actor_loss\": aloss,\n",
    "        \"entropy\": entropy,\n",
    "    }\n",
    "    return train_state, update_info\n",
    "\n",
    "\n",
    "# for evaluation (evaluate for N consecutive episodes, sum rewards)\n",
    "# N=1 single task, N>1 for meta-RL\n",
    "class RolloutStats(struct.PyTreeNode):\n",
    "    reward: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0.0))\n",
    "    length: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0))\n",
    "    episodes: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0))\n",
    "    solved: jax.Array = struct.field(default_factory=lambda: jnp.asarray(0))\n",
    "\n",
    "\n",
    "# for tracking per-episode statistics during meta-RL evaluation\n",
    "class MetaRolloutStats(struct.PyTreeNode):\n",
    "    episode_rewards: jax.Array  # Shape: [max_episodes] - reward for each episode\n",
    "    episode_lengths: jax.Array  # Shape: [max_episodes] - length of each episode\n",
    "    episode_solved: jax.Array  # Shape: [max_episodes] - whether each episode was solved\n",
    "    total_reward: jax.Array  # Scalar - total reward across all episodes\n",
    "    num_episodes_completed: jax.Array  # Scalar - number of episodes actually completed\n",
    "\n",
    "\n",
    "def rollout(\n",
    "    rng: jax.Array,\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    eval_puzzle: PushWorldPuzzle,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: jax.Array,\n",
    "    num_consecutive_episodes: int = 1,\n",
    ") -> RolloutStats:\n",
    "    def _cond_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate = carry\n",
    "        return jnp.less(stats.episodes, num_consecutive_episodes)\n",
    "\n",
    "    def _body_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate = carry\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hstate = train_state.apply_fn(\n",
    "            train_state.params,\n",
    "            {\n",
    "                # We add single channel dimension to end of obs_img\n",
    "                \"obs_img\": timestep.observation[\"img\"][None, None, ...],\n",
    "                \"obs_goal\": timestep.observation[\"goal\"][None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "        timestep = env.step(env_params, timestep, action)\n",
    "\n",
    "        solved_flag = ((timestep.reward == SUCCESS_REWARD) & (timestep.last() == 1)).astype(jnp.int32)\n",
    "        stats = stats.replace(\n",
    "            reward=stats.reward + timestep.reward,\n",
    "            length=stats.length + 1,\n",
    "            episodes=stats.episodes + timestep.last(),\n",
    "            solved=solved_flag,\n",
    "        )\n",
    "        carry = (rng, stats, timestep, action, timestep.reward, hstate)\n",
    "        return carry\n",
    "\n",
    "    env_params = env_params.replace(puzzle=eval_puzzle)\n",
    "    timestep = env.eval_reset(env_params, rng)\n",
    "    prev_action = jnp.asarray(0)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    init_carry = (rng, RolloutStats(), timestep, prev_action, prev_reward, init_hstate)\n",
    "\n",
    "    final_carry = jax.lax.while_loop(_cond_fn, _body_fn, init_val=init_carry)\n",
    "    return final_carry[1]\n",
    "\n",
    "\n",
    "def meta_rollout(\n",
    "    rng: jax.Array,\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    eval_puzzle: PushWorldPuzzle,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: jax.Array,\n",
    "    num_consecutive_episodes: int = 1,\n",
    ") -> MetaRolloutStats:\n",
    "    \"\"\"Rollout that tracks statistics for each individual episode.\"\"\"\n",
    "\n",
    "    def _cond_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate, current_episode_reward, current_episode_length = carry\n",
    "        return jnp.less(stats.num_episodes_completed, num_consecutive_episodes)\n",
    "\n",
    "    def _body_fn(carry):\n",
    "        rng, stats, timestep, prev_action, prev_reward, hstate, current_episode_reward, current_episode_length = carry\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hstate = train_state.apply_fn(\n",
    "            train_state.params,\n",
    "            {\n",
    "                # We add single channel dimension to end of obs_img\n",
    "                \"obs_img\": timestep.observation[\"img\"][None, None, ...],\n",
    "                \"obs_goal\": timestep.observation[\"goal\"][None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "        timestep = env.step(env_params, timestep, action)\n",
    "\n",
    "        # Update current episode accumulators\n",
    "        current_episode_reward = current_episode_reward + timestep.reward\n",
    "        current_episode_length = current_episode_length + 1\n",
    "\n",
    "        # Check if episode ended\n",
    "        episode_ended = timestep.last()\n",
    "        solved_flag = ((timestep.reward == SUCCESS_REWARD) & (episode_ended == 1)).astype(jnp.int32)\n",
    "\n",
    "        # When episode ends, store the episode stats\n",
    "        episode_idx = stats.num_episodes_completed\n",
    "        new_episode_rewards = stats.episode_rewards.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, current_episode_reward, stats.episode_rewards[episode_idx])\n",
    "        )\n",
    "        new_episode_lengths = stats.episode_lengths.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, current_episode_length, stats.episode_lengths[episode_idx])\n",
    "        )\n",
    "        new_episode_solved = stats.episode_solved.at[episode_idx].set(\n",
    "            jnp.where(episode_ended, solved_flag, stats.episode_solved[episode_idx])\n",
    "        )\n",
    "\n",
    "        # Update stats\n",
    "        stats = stats.replace(\n",
    "            episode_rewards=new_episode_rewards,\n",
    "            episode_lengths=new_episode_lengths,\n",
    "            episode_solved=new_episode_solved,\n",
    "            total_reward=stats.total_reward + timestep.reward,\n",
    "            num_episodes_completed=stats.num_episodes_completed + episode_ended,\n",
    "        )\n",
    "\n",
    "        # Reset episode accumulators when episode ends\n",
    "        current_episode_reward = jnp.where(episode_ended, 0.0, current_episode_reward)\n",
    "        current_episode_length = jnp.where(episode_ended, 0, current_episode_length)\n",
    "\n",
    "        carry = (rng, stats, timestep, action, timestep.reward, hstate, current_episode_reward, current_episode_length)\n",
    "        return carry\n",
    "\n",
    "    # Initialize episode tracking arrays\n",
    "    episode_rewards = jnp.zeros(num_consecutive_episodes)\n",
    "    episode_lengths = jnp.zeros(num_consecutive_episodes, dtype=jnp.int32)\n",
    "    episode_solved = jnp.zeros(num_consecutive_episodes, dtype=jnp.int32)\n",
    "\n",
    "    initial_stats = MetaRolloutStats(\n",
    "        episode_rewards=episode_rewards,\n",
    "        episode_lengths=episode_lengths,\n",
    "        episode_solved=episode_solved,\n",
    "        total_reward=jnp.asarray(0.0),\n",
    "        num_episodes_completed=jnp.asarray(0),\n",
    "    )\n",
    "\n",
    "    env_params = env_params.replace(puzzle=eval_puzzle)\n",
    "    timestep = env.eval_reset(env_params, rng)\n",
    "    prev_action = jnp.asarray(0)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    current_episode_reward = jnp.asarray(0.0)\n",
    "    current_episode_length = jnp.asarray(0)\n",
    "\n",
    "    init_carry = (\n",
    "        rng,\n",
    "        initial_stats,\n",
    "        timestep,\n",
    "        prev_action,\n",
    "        prev_reward,\n",
    "        init_hstate,\n",
    "        current_episode_reward,\n",
    "        current_episode_length,\n",
    "    )\n",
    "\n",
    "    final_carry = jax.lax.while_loop(_cond_fn, _body_fn, init_val=init_carry)\n",
    "    return final_carry[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9e235",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12860526",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_threefry_partitionable\", True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    project: str = \"PushWorld\"\n",
    "    group: str = \"default\"\n",
    "    name: str = \"meta-task-ppo\"\n",
    "    benchmark_id: str = \"level0_mini\"\n",
    "    track: bool = False\n",
    "    checkpoint_path: Optional[str] = None\n",
    "    upload_model: bool = False\n",
    "\n",
    "    train_test_same: bool = False\n",
    "    num_train: Optional[int] = None\n",
    "    num_test: Optional[int] = None\n",
    "\n",
    "    img_obs: bool = False\n",
    "    obs_emb_dim: int = 16\n",
    "    action_emb_dim: int = 16\n",
    "    rnn_hidden_dim: int = 1024\n",
    "    rnn_num_layers: int = 1\n",
    "    head_hidden_dim: int = 256\n",
    "    enable_bf16: bool = False\n",
    "    num_envs: int = 8192\n",
    "    num_steps_per_env: int = 4096\n",
    "    num_steps_per_update: int = 32\n",
    "    update_epochs: int = 1\n",
    "    num_minibatches: int = 16\n",
    "    total_timesteps: int = 100_000_000\n",
    "    lr: float = 0.001\n",
    "    clip_eps: float = 0.2\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    eval_num_envs: int = 512\n",
    "    eval_num_episodes: int = 10\n",
    "    eval_seed: int = 42\n",
    "    train_seed: int = 42\n",
    "    puzzle_seed: int = 42\n",
    "    checkpoint_path: Optional[str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        num_devices = jax.local_device_count()\n",
    "        self.num_envs_per_device = self.num_envs // num_devices\n",
    "        self.total_timesteps_per_device = self.total_timesteps // num_devices\n",
    "        self.eval_num_envs_per_device = self.eval_num_envs // num_devices\n",
    "        assert self.num_envs % num_devices == 0\n",
    "        self.num_meta_updates = round(\n",
    "            self.total_timesteps_per_device / (self.num_envs_per_device * self.num_steps_per_env)\n",
    "        )\n",
    "        self.num_inner_updates = self.num_steps_per_env // self.num_steps_per_update\n",
    "        assert self.num_steps_per_env % self.num_steps_per_update == 0\n",
    "        print(f\"Num devices: {num_devices}, Num meta updates: {self.num_meta_updates}\")\n",
    "\n",
    "\n",
    "def make_states(config: TrainConfig):\n",
    "    # for learning rage scheduling\n",
    "    def linear_schedule(count):\n",
    "        total_inner_updates = config.num_minibatches * config.update_epochs * config.num_inner_updates\n",
    "        frac = 1.0 - (count // total_inner_updates) / config.num_meta_updates\n",
    "        return config.lr * frac\n",
    "\n",
    "\n",
    "    env = MetaTaskPushWorldEnvironment()\n",
    "    env_params = env.default_params()\n",
    "    env = GymAutoResetWrapper(env)\n",
    "    env = GoalObservationWrapper(env)\n",
    "\n",
    "\n",
    "    benchmark = pushworld.load_benchmark(config.benchmark_id)\n",
    "\n",
    "    puzzle_rng = jax.random.key(config.puzzle_seed)\n",
    "    train_rng, test_rng = jax.random.split(puzzle_rng)\n",
    "\n",
    "    if config.num_train is not None:\n",
    "        assert (\n",
    "            config.num_train <= benchmark.num_train_puzzles()\n",
    "        ), \"num_train is larger than num train available in benchmark\"\n",
    "        perm = jax.random.permutation(train_rng, benchmark.num_train_puzzles())\n",
    "        idxs = perm[: config.num_train]\n",
    "        benchmark = benchmark.replace(train_puzzles=benchmark.train_puzzles[idxs])\n",
    "    else:\n",
    "        config.num_train = benchmark.num_train_puzzles()\n",
    "\n",
    "    if config.num_test is not None:\n",
    "        assert (\n",
    "            config.num_test <= benchmark.num_test_puzzles()\n",
    "        ), \"num_test is larger than num test available in benchmark\"\n",
    "        perm = jax.random.permutation(test_rng, benchmark.num_test_puzzles())\n",
    "        idxs = perm[: config.num_test]\n",
    "        benchmark = benchmark.replace(test_puzzles=benchmark.test_puzzles[idxs])\n",
    "    else:\n",
    "        config.num_test = benchmark.num_test_puzzles()\n",
    "\n",
    "    if config.train_test_same:\n",
    "        benchmark = benchmark.replace(test_puzzles=benchmark.train_puzzles)\n",
    "        config.num_test = config.num_train\n",
    "\n",
    "    rng = jax.random.key(config.train_seed)\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    network = ActorCriticRNN(\n",
    "        num_actions=env.num_actions(env_params),\n",
    "        obs_emb_dim=config.obs_emb_dim,\n",
    "        action_emb_dim=config.action_emb_dim,\n",
    "        rnn_hidden_dim=config.rnn_hidden_dim,\n",
    "        rnn_num_layers=config.rnn_num_layers,\n",
    "        head_hidden_dim=config.head_hidden_dim,\n",
    "        img_obs=config.img_obs,\n",
    "        dtype=jnp.bfloat16 if config.enable_bf16 else None,\n",
    "    )\n",
    "    shapes = env.observation_shape(env_params)\n",
    "\n",
    "    init_obs = {\n",
    "        # We add single channel dimension to end of obs_img\n",
    "        \"obs_img\": jnp.zeros((config.num_envs_per_device, 1, *shapes[\"img\"])),\n",
    "        \"obs_goal\": jnp.zeros((config.num_envs_per_device, 1, shapes[\"goal\"])),\n",
    "        \"prev_action\": jnp.zeros((config.num_envs_per_device, 1), dtype=jnp.int32),\n",
    "        \"prev_reward\": jnp.zeros((config.num_envs_per_device, 1)),\n",
    "    }\n",
    "    init_hstate = network.initialize_carry(batch_size=config.num_envs_per_device)\n",
    "\n",
    "    network_params = network.init(_rng, init_obs, init_hstate)\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(config.max_grad_norm),\n",
    "        optax.inject_hyperparams(optax.adam)(learning_rate=linear_schedule, eps=1e-8),  # eps=1e-5\n",
    "    )\n",
    "    train_state = TrainState.create(apply_fn=network.apply, params=network_params, tx=tx)\n",
    "\n",
    "    return rng, env, env_params, benchmark, init_hstate, train_state\n",
    "\n",
    "\n",
    "def make_train(\n",
    "    env: Environment,\n",
    "    env_params: EnvParams,\n",
    "    benchmark: Benchmark,\n",
    "    config: TrainConfig,\n",
    "):\n",
    "    @partial(jax.pmap, axis_name=\"devices\")\n",
    "    def train(\n",
    "        rng: jax.Array,\n",
    "        train_state: TrainState,\n",
    "        init_hstate: jax.Array,\n",
    "    ):\n",
    "        eval_hstate = init_hstate[0][None]\n",
    "\n",
    "        # META TRAIN LOOP\n",
    "        def _meta_step(meta_state, _):\n",
    "            rng, train_state = meta_state\n",
    "\n",
    "            rng, _rng1, _rng2 = jax.random.split(rng, num=3)\n",
    "            puzzle_rng = jax.random.split(_rng1, num=config.num_envs_per_device)\n",
    "            reset_rng = jax.random.split(_rng2, num=config.num_envs_per_device)\n",
    "\n",
    "            puzzles = jax.vmap(benchmark.sample_puzzle, in_axes=(0, None))(puzzle_rng, \"train\")\n",
    "            meta_env_params = env_params.replace(puzzle=puzzles)\n",
    "\n",
    "            timestep = jax.vmap(env.reset, in_axes=(0, 0))(meta_env_params, reset_rng)\n",
    "            prev_action = jnp.zeros(config.num_envs_per_device, dtype=jnp.int32)\n",
    "            prev_reward = jnp.zeros(config.num_envs_per_device)\n",
    "\n",
    "            # INNER TRAIN LOOP\n",
    "            def _update_step(runner_state, _):\n",
    "                # COLLECT TRAJECTORIES\n",
    "                def _env_step(runner_state, _):\n",
    "                    rng, train_state, prev_timestep, prev_action, prev_reward, prev_hstate = runner_state\n",
    "\n",
    "                    rng, _rng = jax.random.split(rng)\n",
    "                    dist, value, hstate = train_state.apply_fn(\n",
    "                        train_state.params,\n",
    "                        {\n",
    "                            # [batch_size, seq_len=1, ...]\n",
    "                            # We add single channel dimension to end of obs_img\n",
    "                            \"obs_img\": prev_timestep.observation[\"img\"][:, None],\n",
    "                            \"obs_goal\": prev_timestep.observation[\"goal\"][:, None],\n",
    "                            \"prev_action\": prev_action[:, None],\n",
    "                            \"prev_reward\": prev_reward[:, None],\n",
    "                        },\n",
    "                        prev_hstate,\n",
    "                    )\n",
    "                    action, log_prob = dist.sample_and_log_prob(seed=_rng)\n",
    "                    action, value, log_prob = action.squeeze(1), value.squeeze(1), log_prob.squeeze(1)\n",
    "\n",
    "                    timestep = jax.vmap(env.step, in_axes=0)(meta_env_params, prev_timestep, action)\n",
    "                    transition = Transition(\n",
    "                        # ATTENTION: done is always false, as we optimize for entire meta-rollout\n",
    "                        done=jnp.zeros_like(timestep.last()),\n",
    "                        action=action,\n",
    "                        value=value,\n",
    "                        reward=timestep.reward,\n",
    "                        log_prob=log_prob,\n",
    "                        obs=prev_timestep.observation[\"img\"],\n",
    "                        goal=prev_timestep.observation[\"goal\"],\n",
    "                        prev_action=prev_action,\n",
    "                        prev_reward=prev_reward,\n",
    "                    )\n",
    "                    runner_state = (rng, train_state, timestep, action, timestep.reward, hstate)\n",
    "                    return runner_state, transition\n",
    "\n",
    "                initial_hstate = runner_state[-1]\n",
    "                runner_state, transitions = jax.lax.scan(_env_step, runner_state, None, config.num_steps_per_update)\n",
    "\n",
    "                rng, train_state, timestep, prev_action, prev_reward, hstate = runner_state\n",
    "                _, last_val, _ = train_state.apply_fn(\n",
    "                    train_state.params,\n",
    "                    {\n",
    "                        # We add single channel dimension to end of obs_img\n",
    "                        \"obs_img\": timestep.observation[\"img\"][:, None],\n",
    "                        \"obs_goal\": timestep.observation[\"goal\"][:, None],\n",
    "                        \"prev_action\": prev_action[:, None],\n",
    "                        \"prev_reward\": prev_reward[:, None],\n",
    "                    },\n",
    "                    hstate,\n",
    "                )\n",
    "                advantages, targets = calculate_gae(transitions, last_val.squeeze(1), config.gamma, config.gae_lambda)\n",
    "\n",
    "                # UPDATE NETWORK\n",
    "                def _update_epoch(update_state, _):\n",
    "                    def _update_minbatch(train_state, batch_info):\n",
    "                        init_hstate, transitions, advantages, targets = batch_info\n",
    "                        new_train_state, update_info = ppo_update_networks(\n",
    "                            train_state=train_state,\n",
    "                            transitions=transitions,\n",
    "                            init_hstate=init_hstate.squeeze(1),\n",
    "                            advantages=advantages,\n",
    "                            targets=targets,\n",
    "                            clip_eps=config.clip_eps,\n",
    "                            vf_coef=config.vf_coef,\n",
    "                            ent_coef=config.ent_coef,\n",
    "                        )\n",
    "                        return new_train_state, update_info\n",
    "\n",
    "                    rng, train_state, init_hstate, transitions, advantages, targets = update_state\n",
    "\n",
    "                    rng, _rng = jax.random.split(rng)\n",
    "                    permutation = jax.random.permutation(_rng, config.num_envs_per_device)\n",
    "                    batch = (init_hstate, transitions, advantages, targets)\n",
    "                    batch = jtu.tree_map(lambda x: x.swapaxes(0, 1), batch)\n",
    "\n",
    "                    shuffled_batch = jtu.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "                    minibatches = jtu.tree_map(\n",
    "                        lambda x: jnp.reshape(x, (config.num_minibatches, -1) + x.shape[1:]), shuffled_batch\n",
    "                    )\n",
    "                    train_state, update_info = jax.lax.scan(_update_minbatch, train_state, minibatches)\n",
    "\n",
    "                    update_state = (rng, train_state, init_hstate, transitions, advantages, targets)\n",
    "                    return update_state, update_info\n",
    "\n",
    "                update_state = (rng, train_state, initial_hstate[None, :], transitions, advantages, targets)\n",
    "                update_state, loss_info = jax.lax.scan(_update_epoch, update_state, None, config.update_epochs)\n",
    "                rng, train_state = update_state[:2]\n",
    "\n",
    "                loss_info = jtu.tree_map(lambda x: x.mean(-1).mean(-1), loss_info)\n",
    "                runner_state = (rng, train_state, timestep, prev_action, prev_reward, hstate)\n",
    "                return runner_state, loss_info\n",
    "\n",
    "            runner_state = (rng, train_state, timestep, prev_action, prev_reward, init_hstate)\n",
    "            runner_state, loss_info = jax.lax.scan(_update_step, runner_state, None, config.num_inner_updates)\n",
    "            rng, train_state = runner_state[:2]\n",
    "\n",
    "            eval_reset_rng = jax.random.key(config.eval_seed)\n",
    "            eval_test_rng, eval_train_rng = jax.random.split(eval_reset_rng)\n",
    "            assert config.num_test is not None, \"num_test must be set for evaluation\"\n",
    "            assert config.num_train is not None, \"num_train must be set for evaluation\"\n",
    "\n",
    "            eval_test_reset_rng = jax.random.split(eval_test_rng, num=config.num_test)\n",
    "            eval_test_puzzles = benchmark.get_test_puzzles()\n",
    "            eval_test_stats = jax.vmap(meta_rollout, in_axes=(0, None, None, 0, None, None, None))(\n",
    "                eval_test_reset_rng,\n",
    "                env,\n",
    "                meta_env_params,\n",
    "                eval_test_puzzles,\n",
    "                train_state,\n",
    "                eval_hstate,\n",
    "                config.eval_num_episodes,\n",
    "            )\n",
    "            eval_test_stats = jax.lax.pmean(eval_test_stats, axis_name=\"devices\")\n",
    "\n",
    "            eval_train_reset_rng = jax.random.split(eval_train_rng, num=config.num_train)\n",
    "            eval_train_puzzles = benchmark.get_train_puzzles()\n",
    "            eval_train_stats = jax.vmap(meta_rollout, in_axes=(0, None, None, 0, None, None, None))(\n",
    "                eval_train_reset_rng,\n",
    "                env,\n",
    "                meta_env_params,\n",
    "                eval_train_puzzles,\n",
    "                train_state,\n",
    "                eval_hstate,\n",
    "                config.eval_num_episodes,\n",
    "            )\n",
    "            eval_train_stats = jax.lax.pmean(eval_train_stats, axis_name=\"devices\")\n",
    "\n",
    "            loss_info = jtu.tree_map(lambda x: x.mean(-1), loss_info)\n",
    "            loss_info.update(\n",
    "                {\n",
    "                    # Originally we divided returns_mean by config.eval_num_episodes, but we realized\n",
    "                    # that it is more intuitive to think about the cumulative returns over the whole meta-episode.\n",
    "                    \"eval_test/returns_mean\": eval_test_stats.total_reward.mean(),\n",
    "                    \"eval_train/returns_mean_train\": eval_train_stats.total_reward.mean(),\n",
    "                    \"eval_test/returns_median\": jnp.median(eval_test_stats.total_reward),\n",
    "                    \"eval_test/returns_20percentile\": jnp.percentile(eval_test_stats.total_reward, q=20),\n",
    "                    # Our definition of solved is whether the agent solves the last trial in the meta-episode.\n",
    "                    \"eval_test/solved_percentage\": eval_test_stats.episode_solved[:, -1].mean(),\n",
    "                    \"eval_train/solved_percentage_train\": eval_train_stats.episode_solved[:, -1].mean(),\n",
    "                    \"eval_test/lengths\": eval_test_stats.episode_lengths.mean(),\n",
    "                    \"eval_test/lengths_20percentile\": jnp.percentile(eval_test_stats.episode_lengths, q=20),\n",
    "                    \"lr\": train_state.opt_state[-1].hyperparams[\"learning_rate\"],\n",
    "                    # Store episode arrays - we'll convert to individual metrics outside JIT\n",
    "                    \"eval_test/episode_rewards\": eval_test_stats.episode_rewards.mean(axis=0),\n",
    "                    \"eval_test/episode_solved_rates\": eval_test_stats.episode_solved.mean(axis=0),\n",
    "                    \"eval_train/episode_rewards\": eval_train_stats.episode_rewards.mean(axis=0),\n",
    "                    \"eval_train/episode_solved_rates\": eval_train_stats.episode_solved.mean(axis=0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            meta_state = (rng, train_state)\n",
    "            return meta_state, loss_info\n",
    "\n",
    "        meta_state = (rng, train_state)\n",
    "        meta_state, loss_info = jax.lax.scan(_meta_step, meta_state, None, config.num_meta_updates)\n",
    "        return {\"state\": meta_state[-1], \"loss_info\": loss_info}\n",
    "\n",
    "    return train\n",
    "\n",
    "\n",
    "def train(config: TrainConfig):\n",
    "    # removing existing checkpoints if any\n",
    "    if config.checkpoint_path is not None and os.path.exists(config.checkpoint_path):\n",
    "        shutil.rmtree(config.checkpoint_path)\n",
    "\n",
    "    rng, env, env_params, benchmark, init_hstate, train_state = make_states(config)\n",
    "    rng = jax.random.split(rng, num=jax.local_device_count())\n",
    "    train_state = replicate(train_state, jax.local_devices())\n",
    "    init_hstate = replicate(init_hstate, jax.local_devices())\n",
    "\n",
    "    print(\"Compiling...\")\n",
    "    t = time.time()\n",
    "    train_fn = make_train(env, env_params, benchmark, config)\n",
    "    train_fn = train_fn.lower(rng, train_state, init_hstate).compile()\n",
    "    elapsed_time = time.time() - t\n",
    "    print(f\"Done in {elapsed_time:.2f}s.\")\n",
    "\n",
    "    print(\"Training...\")\n",
    "    t = time.time()\n",
    "    train_info = jax.block_until_ready(train_fn(rng, train_state, init_hstate))\n",
    "    elapsed_time = time.time() - t\n",
    "    print(f\"Done in {elapsed_time:.2f}s.\")\n",
    "\n",
    "    return unreplicate(train_info), elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc94c1c",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ca18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_threefry_partitionable\", True)\n",
    "\n",
    "\n",
    "def processing(config: TrainConfig, train_info, elapsed_time):\n",
    "    print(\"Logginig...\")\n",
    "    loss_info = train_info[\"loss_info\"]\n",
    "\n",
    "    if config.track or config.upload_model:\n",
    "        run = wandb.init(\n",
    "            project=config.project,\n",
    "            group=config.group,\n",
    "            name=config.name,\n",
    "            config=asdict(config),\n",
    "            save_code=True,\n",
    "        )\n",
    "\n",
    "    if config.track:\n",
    "        total_transitions = 0\n",
    "\n",
    "        # I want to manipulate episode_rewards and episode_solved_rates so that I generate\n",
    "        # eval_num_episodes separate metrics, where each metric represents the returns or solved rate\n",
    "        # for one episode in the meta-episode, over meta_updates.\n",
    "        for episode_idx in range(config.eval_num_episodes):\n",
    "            loss_info[f\"eval_test/episode_rewards/{episode_idx}\"] = loss_info[\"eval_test/episode_rewards\"].swapaxes(\n",
    "                0, 1\n",
    "            )[episode_idx]\n",
    "            loss_info[f\"eval_test/episode_solved_rates/{episode_idx}\"] = loss_info[\n",
    "                \"eval_test/episode_solved_rates\"\n",
    "            ].swapaxes(0, 1)[episode_idx]\n",
    "            loss_info[f\"eval_train/episode_rewards/{episode_idx}\"] = loss_info[\"eval_train/episode_rewards\"].swapaxes(\n",
    "                0, 1\n",
    "            )[episode_idx]\n",
    "            loss_info[f\"eval_train/episode_solved_rates/{episode_idx}\"] = loss_info[\n",
    "                \"eval_train/episode_solved_rates\"\n",
    "            ].swapaxes(0, 1)[episode_idx]\n",
    "\n",
    "        loss_info.pop(\"eval_test/episode_rewards\")\n",
    "        loss_info.pop(\"eval_test/episode_solved_rates\")\n",
    "        loss_info.pop(\"eval_train/episode_rewards\")\n",
    "        loss_info.pop(\"eval_train/episode_solved_rates\")\n",
    "\n",
    "        for i in range(config.num_meta_updates):\n",
    "            total_transitions += config.num_steps_per_env * config.num_envs_per_device * jax.local_device_count()\n",
    "            info = jtu.tree_map(lambda x: x[i].item(), loss_info)\n",
    "            info[\"transitions\"] = total_transitions\n",
    "            wandb.log(info)\n",
    "\n",
    "        run.summary[\"training_time\"] = elapsed_time\n",
    "        run.summary[\"steps_per_second\"] = (config.total_timesteps_per_device * jax.local_device_count()) / elapsed_time\n",
    "\n",
    "    if config.checkpoint_path is not None:\n",
    "        checkpoint = {\"config\": asdict(config), \"params\": train_info[\"state\"].params}\n",
    "        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        save_args = orbax_utils.save_args_from_target(checkpoint)\n",
    "        orbax_checkpointer.save(config.checkpoint_path, checkpoint, save_args=save_args)\n",
    "\n",
    "        if config.upload_model:\n",
    "            artifact = wandb.Artifact(\n",
    "                name=f\"model-checkpoint-{run.id}\", type=\"model\", description=\"Trained model checkpoint\"\n",
    "            )\n",
    "            artifact.add_dir(config.checkpoint_path)  # Add entire checkpoint directory\n",
    "            run.log_artifact(artifact)\n",
    "\n",
    "    if config.track or config.upload_model:\n",
    "        run.finish()\n",
    "\n",
    "    print(\"Final test return: \", float(loss_info[\"eval_test/returns_mean\"][-1]))\n",
    "    print(\"Final train return: \", float(loss_info[\"eval_train/returns_mean_train\"][-1]))\n",
    "    print(\"Final test solve rate: \", float(loss_info[\"eval_test/solved_percentage\"][-1]))\n",
    "    print(\"Final train solve rate: \", float(loss_info[\"eval_train/solved_percentage_train\"][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93087a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378436ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_string: str):\n",
    "    \"\"\"Converts a standard 6-digit hex color into a tuple of decimal\n",
    "    (red, green, blue) values.\"\"\"\n",
    "    return tuple(int(hex_string[i : i + 2], 16) for i in (0, 2, 4))\n",
    "\n",
    "\n",
    "symbol_to_rgb = {\n",
    "    0: hex_to_rgb(\"FFFFFF\"),  # empty  white\n",
    "    1: hex_to_rgb(\"00DC00\"),  # agent  \"00DC00\"\n",
    "    2: hex_to_rgb(\"469BFF\"),  # movable  \"469BFF\"\n",
    "    3: hex_to_rgb(\"DC0000\"),  # movable_goal  \"DC0000\"\n",
    "    4: hex_to_rgb(\"0A0A0A\"),  # wall  \"0A0A0A\"\n",
    "}\n",
    "\n",
    "\n",
    "def text_to_rgb(goal_pos, grid):\n",
    "    \"\"\"grid: 2-D array of str, shape (H, W)\"\"\"\n",
    "    h, w = grid.shape\n",
    "    img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for sym, rgb in symbol_to_rgb.items():\n",
    "        mask = grid == sym\n",
    "        img[mask] = rgb\n",
    "\n",
    "    if grid[goal_pos[1], goal_pos[0]] == Tiles.EMPTY:\n",
    "        img[goal_pos[1], goal_pos[0]] = hex_to_rgb(\"FF7F7F\")  # light red\n",
    "\n",
    "    # upscale (optional) so each tile is, say, 1616 pixels\n",
    "    img = np.kron(img, np.ones((64, 64, 1), dtype=np.uint8))\n",
    "    return img\n",
    "\n",
    "\n",
    "def text_to_rgb_all(observation: jax.Array):\n",
    "    # I want you to render the observation into a grid\n",
    "    # Observation is a jax.Array, shape (H, W, 8),\n",
    "    # Where 8 is the number of channels.\n",
    "    # Each channel represents a different object, which should have its own color.\n",
    "    # This is the order of the channels:\n",
    "    # channels.append(create_channel(state.a))  # agent\n",
    "    # channels.append(create_channel(state.m1))  # movable 1\n",
    "    # channels.append(create_channel(state.m2))  # movable 2\n",
    "    # channels.append(create_channel(state.m3))  # movable 3\n",
    "    # channels.append(create_channel(state.m4))  # movable 4\n",
    "    # channels.append(create_channel(puzzle.g1))  # goal 1\n",
    "    # channels.append(create_channel(puzzle.g2))  # goal 2\n",
    "    # channels.append(create_channel(puzzle.w))  # walls\n",
    "\n",
    "    # Movables that have associated goals should be given the \"movable_goal\" color,\n",
    "    # movables that do not should just be given the \"movable\" color.\n",
    "\n",
    "    # Convert to numpy for easier processing\n",
    "    obs_np = np.array(observation)\n",
    "    h, w = obs_np.shape[:2]\n",
    "\n",
    "    # Create RGB image initialized to white (empty spaces)\n",
    "    rgb_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    rgb_img.fill(255)  # white background\n",
    "\n",
    "    # Channel indices\n",
    "    AGENT_CH = 0\n",
    "    M1_CH, M2_CH, M3_CH, M4_CH = 1, 2, 3, 4\n",
    "    G1_CH, G2_CH = 5, 6\n",
    "    WALL_CH = 7\n",
    "\n",
    "    # Extract individual channels\n",
    "    agent = obs_np[:, :, AGENT_CH]\n",
    "    m1 = obs_np[:, :, M1_CH]\n",
    "    m2 = obs_np[:, :, M2_CH]\n",
    "    m3 = obs_np[:, :, M3_CH]\n",
    "    m4 = obs_np[:, :, M4_CH]\n",
    "    g1 = obs_np[:, :, G1_CH]\n",
    "    g2 = obs_np[:, :, G2_CH]\n",
    "    walls = obs_np[:, :, WALL_CH]\n",
    "\n",
    "    # Render walls first (bottom layer)\n",
    "    wall_mask = walls > 0\n",
    "    rgb_img[wall_mask] = symbol_to_rgb[4]  # black\n",
    "\n",
    "    # Render goals (light red for empty goals)\n",
    "    g1_mask = g1 > 0\n",
    "    g2_mask = g2 > 0\n",
    "    rgb_img[g1_mask] = hex_to_rgb(\"FF7F7F\")  # light red\n",
    "    rgb_img[g2_mask] = hex_to_rgb(\"FF7F7F\")  # light red\n",
    "\n",
    "    # Render movables with appropriate colors\n",
    "    # Goal-movable pairing is dynamic based on which goals exist:\n",
    "    # - If only g1 exists: m1 is the goal movable, m2/m3/m4 are regular movables\n",
    "    # - If both g1 and g2 exist: m1 and m2 are goal movables, m3/m4 are regular movables\n",
    "\n",
    "    # Check which goals exist\n",
    "    g1_exists = np.any(g1_mask)\n",
    "    g2_exists = np.any(g2_mask)\n",
    "\n",
    "    # m1: movable_goal color if on g1. g1 is guaranteed to always exist.\n",
    "    m1_mask = m1 > 0\n",
    "    rgb_img[m1_mask] = symbol_to_rgb[3]  # movable_goal (red)\n",
    "\n",
    "    # m2: movable_goal color if on g2 (when g2 exists), otherwise movable color\n",
    "    m2_mask = m2 > 0\n",
    "    if g2_exists:\n",
    "        rgb_img[m2_mask] = symbol_to_rgb[3]  # movable_goal (red)\n",
    "    else:\n",
    "        rgb_img[m2_mask] = symbol_to_rgb[2]  # movable (blue)\n",
    "\n",
    "    # m3 and m4: always regular movable color (no associated goals)\n",
    "    m3_mask = m3 > 0\n",
    "    m4_mask = m4 > 0\n",
    "    rgb_img[m3_mask] = symbol_to_rgb[2]  # movable (blue)\n",
    "    rgb_img[m4_mask] = symbol_to_rgb[2]  # movable (blue)\n",
    "\n",
    "    # Render agent on top\n",
    "    agent_mask = agent > 0\n",
    "    rgb_img[agent_mask] = symbol_to_rgb[1]  # agent (green)\n",
    "\n",
    "    # Upscale for better visibility (64x64 pixels per tile)\n",
    "    upscaled_img = np.kron(rgb_img, np.ones((64, 64, 1), dtype=np.uint8))\n",
    "\n",
    "    return upscaled_img\n",
    "\n",
    "\n",
    "def fetch_model_from_wandb(\n",
    "    project: str,\n",
    "    run_id: str,\n",
    "    artifact_name: Optional[str] = None,\n",
    "    download_path: str = \"./downloaded_checkpoints\",\n",
    "    entity: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch model parameters from a WandB artifact.\n",
    "\n",
    "    Args:\n",
    "        project: WandB project name\n",
    "        run_id: WandB run ID\n",
    "        artifact_name: Specific artifact name (if None, uses f\"model-checkpoint-{run_id}\")\n",
    "        download_path: Local path to download the artifact\n",
    "        entity: WandB entity/username (if None, uses default)\n",
    "\n",
    "    Returns:\n",
    "        Model parameters from the checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize wandb in offline mode to avoid creating a new run\n",
    "    if entity:\n",
    "        wandb.init(project=project, entity=entity, mode=\"offline\")\n",
    "    else:\n",
    "        wandb.init(project=project, mode=\"offline\")\n",
    "\n",
    "    try:\n",
    "        # Construct artifact name if not provided\n",
    "        if artifact_name is None:\n",
    "            artifact_name = f\"model-checkpoint-{run_id}:latest\"\n",
    "        elif \":latest\" not in artifact_name and \":v\" not in artifact_name:\n",
    "            artifact_name = f\"{artifact_name}:latest\"\n",
    "\n",
    "        print(f\"Fetching artifact: {artifact_name}\")\n",
    "\n",
    "        # Download the artifact\n",
    "        artifact = wandb.use_artifact(artifact_name)\n",
    "        artifact_dir = artifact.download(root=download_path)\n",
    "\n",
    "        print(f\"Downloaded to: {artifact_dir}\")\n",
    "\n",
    "        # Load the checkpoint using orbax\n",
    "        orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        checkpoint = orbax_checkpointer.restore(artifact_dir)\n",
    "\n",
    "        print(\"Successfully loaded checkpoint\")\n",
    "\n",
    "        return checkpoint[\"params\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching model from WandB: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407eee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_meta(train_info, config, puzzles, video_name, eval_seed):\n",
    "    META_EPISODES = 10\n",
    "    # We're only going to sample from test anyway\n",
    "    benchmark = Benchmark(\n",
    "        train_puzzles=puzzles,\n",
    "        test_puzzles=puzzles,\n",
    "    )\n",
    "\n",
    "    # setup environment\n",
    "    env = MetaTaskPushWorldEnvironment()\n",
    "    env = GymAutoResetWrapper(env)\n",
    "    env = GoalObservationWrapper(env)\n",
    "    env_params = env.default_params()\n",
    "\n",
    "    rng = jax.random.key(eval_seed)\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    puzzle = benchmark.sample_puzzle(_rng, \"test\")\n",
    "    env_params = env_params.replace(puzzle=puzzle)\n",
    "\n",
    "    # you can use train_state from the final state also\n",
    "    # we just demo here how to do it if you loaded params from the checkpoint\n",
    "    params = train_info[\"state\"].params\n",
    "    model = ActorCriticRNN(\n",
    "        num_actions=env.num_actions(env_params),\n",
    "        action_emb_dim=config.action_emb_dim,\n",
    "        rnn_hidden_dim=config.rnn_hidden_dim,\n",
    "        rnn_num_layers=config.rnn_num_layers,\n",
    "        head_hidden_dim=config.head_hidden_dim,\n",
    "        img_obs=config.img_obs,\n",
    "    )\n",
    "\n",
    "    # jitting all functions\n",
    "    apply_fn, reset_fn, step_fn = jax.jit(model.apply), jax.jit(env.reset), jax.jit(env.step)\n",
    "\n",
    "    # for logging\n",
    "    total_reward, num_episodes = 0, 0\n",
    "    rendered_imgs = []\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    # initial inputs\n",
    "    hidden = model.initialize_carry(1)\n",
    "    prev_reward = jnp.asarray(0)\n",
    "    prev_action = jnp.asarray(0)\n",
    "\n",
    "    timestep = reset_fn(env_params, _rng)\n",
    "    rendered_imgs.append(text_to_rgb(timestep.state.goal_pos, timestep.observation[\"img\"].squeeze(-1)))\n",
    "\n",
    "    while num_episodes < META_EPISODES:\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        dist, _, hidden = apply_fn(\n",
    "            params,\n",
    "            {\n",
    "                \"obs_img\": timestep.observation[\"img\"][None, None, ...],\n",
    "                \"obs_goal\": timestep.observation[\"goal\"][None, None, ...],\n",
    "                \"prev_action\": prev_action[None, None, ...],\n",
    "                \"prev_reward\": prev_reward[None, None, ...],\n",
    "            },\n",
    "            hidden,\n",
    "        )\n",
    "        action = dist.sample(seed=_rng).squeeze()\n",
    "\n",
    "        timestep = step_fn(env_params, timestep, action)\n",
    "        prev_action = action\n",
    "        prev_reward = timestep.reward\n",
    "\n",
    "        total_reward += timestep.reward.item()\n",
    "        num_episodes += int(timestep.last().item())\n",
    "        rendered_imgs.append(text_to_rgb(timestep.state.goal_pos, timestep.observation[\"img\"].squeeze(-1)))\n",
    "\n",
    "    imageio.mimsave(f\"{video_name}.mp4\", rendered_imgs, fps=16, format=\"mp4\")\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3d07d",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\" # fill in\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"], relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    benchmark_id=\"level0_transformed_base\",\n",
    "    total_timesteps=1000_000_000,\n",
    "    num_envs=8192,\n",
    "    num_steps_per_env=500,\n",
    "    num_steps_per_update=500,\n",
    "    train_test_same=False,\n",
    "    num_train=2000,\n",
    "    num_test=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a907a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info, elapsed_time = train(config)\n",
    "processing(config, train_info, elapsed_time)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
