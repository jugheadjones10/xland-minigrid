{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran into a subtle leaked Tracer bug while trying to vmap my training function across different values of learning rate. The training function contains the end-to-end training process for classic PPO. We use optax for our optimization. In the original code, we created a linearly decaying schedule for the learning rate and then passed that to optax's TrainState, which we initialized outside of the training function. We then passed the TrainState object to the training function.\n",
    "\n",
    "In order to vmap across learning rates, I had to move the TrainState initialization into the training function so that we could create a different schedule for each learning rate value. When I ran the code, I got this error:\n",
    "\n",
    "```\n",
    "Exception: Leaked trace BatchTrace\n",
    "```\n",
    "\n",
    "Here is a code snippet that reproduces the error. Instead of using optax's TrainState, we create a custom pytree node to mimic its behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Leaked trace BatchTrace. Leaked tracer(s):\n\nTraced<ShapedArray(float32[])>with<BatchTrace> with\n  val = Array([ 1.,  5., 10.], dtype=float32)\n  batch_dim = 0\nThis BatchTracer with object id 4816883200 was created on line:\n  /var/folders/q4/2lsmb6qd1ks8137720rg8fz80000gn/T/ipykernel_27631/527263614.py:34:4 (<module>)\n<BatchTracer 4816883200> is referred to by <function 4822615680> (learning_schedule) closed-over variable lr\n<function 4822615680> is referred to by <TrainState 4822757392>.fn\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     29\u001b[39m vmapped_creator = jax.vmap(train_fn)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.checking_leaks():\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# 4. Now the `TypeError` is bypassed, and JAX's leak detection\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m#    finds the tracer inside the container, raising the intended error.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43mvmapped_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 4 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.4/lib/python3.11/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/monkey/xland-minigrid/.venv/lib/python3.11/site-packages/jax/_src/core.py:1234\u001b[39m, in \u001b[36mensure_no_leaks\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1232\u001b[39m leaked_tracers = maybe_find_leaked_tracers(live_trace)\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m leaked_tracers:\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m leaked_tracer_error(\u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m, live_trace, leaked_tracers)\n",
      "\u001b[31mException\u001b[39m: Leaked trace BatchTrace. Leaked tracer(s):\n\nTraced<ShapedArray(float32[])>with<BatchTrace> with\n  val = Array([ 1.,  5., 10.], dtype=float32)\n  batch_dim = 0\nThis BatchTracer with object id 4816883200 was created on line:\n  /var/folders/q4/2lsmb6qd1ks8137720rg8fz80000gn/T/ipykernel_27631/527263614.py:34:4 (<module>)\n<BatchTracer 4816883200> is referred to by <function 4822615680> (learning_schedule) closed-over variable lr\n<function 4822615680> is referred to by <TrainState 4822757392>.fn\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import tree_util\n",
    "\n",
    "\n",
    "class TrainState:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "\n",
    "\n",
    "tree_util.register_pytree_node(\n",
    "    TrainState,\n",
    "    lambda container: ((), container),\n",
    "    lambda aux_data, children: aux_data,\n",
    ")\n",
    "\n",
    "\n",
    "def train_fn(lr: jax.Array):\n",
    "    def learning_schedule():\n",
    "        return lr * 10.0\n",
    "\n",
    "    return TrainState(fn=learning_schedule)\n",
    "\n",
    "\n",
    "def train_fn_b(lr: jax.Array):\n",
    "    return lr * 10.0\n",
    "\n",
    "\n",
    "# An array of values to map over\n",
    "input_values = jnp.array([1.0, 5.0, 10.0])\n",
    "\n",
    "# vmap the creator function\n",
    "vmapped_creator = jax.vmap(train_fn)\n",
    "\n",
    "with jax.checking_leaks():\n",
    "    # 4. Now the `TypeError` is bypassed, and JAX's leak detection\n",
    "    #    finds the tracer inside the container, raising the intended error.\n",
    "    vmapped_creator(input_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we vmap `train_fn`, Jax passes a BatchedTracer through `train_fn` during the tracing stage. Within `train_fn`, lr (currently a BatchedTracer) is captured in the closure of `learning_schedule`. When we initialize `TrainState` by passing in `learning_schedule`, we are in a way \"storing\" a BatchedTracer in the `TrainState` object. That's fine if `TrainState` remains in the function, but the problem here is that we are returning it from the function. This causes the BatchedTracer to be leaked.\n",
    "\n",
    "\"But tracing a function will return Tracers anyway!\" you might say. Well, yes, we return Tracers, but that's for when we do operations like `* 5.0` or `+ 1.0` to the input Tracer. Storing the Tracer inside a Python object is not an operation that is \"supported\" by Tracers.\n",
    "\n",
    "One gotcha is that we don't gt this error if we don't use `jax.tracing_leaks()`. Seems like Jax doesn't do these checks by default?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
